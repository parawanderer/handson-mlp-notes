{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 10 – Building Neural Networks with PyTorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in chapter 10._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ageron/handson-mlp/blob/main/10_neural_nets_with_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-mlp/blob/main/10_neural_nets_with_pytorch.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires Python 3.10 or above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also requires Scikit-Learn ≥ 1.6.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging.version import Version\n",
    "import sklearn\n",
    "\n",
    "assert Version(sklearn.__version__) >= Version(\"1.6.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we using Colab or Kaggle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Colab, a couple libraries are not pre-installed so we must install them manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "    %pip install -q optuna torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course we need PyTorch, specifically PyTorch ≥ 2.6.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert Version(torch.__version__) >= Version(\"2.6.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in earlier chapters, let's define the default font sizes to make the figures prettier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mermaid import Mermaid\n",
    "\n",
    "def mermaid_diagram(diagram: str):\n",
    "    return Mermaid(f\"\"\"\n",
    "%%{{\n",
    "init: {{\n",
    "        'theme': 'base',\n",
    "        'themeVariables': {{\n",
    "        'primaryColor': '#494f5c',\n",
    "        'primaryTextColor': '#fff',\n",
    "        'primaryBorderColor': '#5f6573',\n",
    "        'lineColor': '#9a9ca1',\n",
    "        'secondaryColor': 'transparent',\n",
    "        'tertiaryColor': '#fff'\n",
    "        }}\n",
    "    }}\n",
    "}}%%\n",
    "\n",
    "{diagram}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Fundamentals\n",
    "## PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 2 x 3 array\n",
    "X = torch.tensor([\n",
    "    [1.0, 4.0, 7.0],\n",
    "    [2.0, 3.0, 6.0]\n",
    "])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 3.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20., 50., 80.],\n",
       "        [30., 40., 70.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 * (X + 1.0)  # item-wise addition and multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2.7183,   54.5981, 1096.6332],\n",
       "        [   7.3891,   20.0855,  403.4288]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8333)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8333333333333335"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 + 2 + 3 + 4 + 6 + 7)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([2., 4., 7.]),\n",
       "indices=tensor([1, 0, 0]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[66., 56.],\n",
       "        [56., 49.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X @ X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 4., 7.],\n",
       "       [2., 3., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array([[1., 4., 7.], [2., 3., 6.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array([[1., 4., 7.], [2., 3., 6.]]), dtype=torch.float32) # Better to call this, as NNs don't need the numpy-default 64-bit precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(np.array([[1., 4., 7.], [2., 3., 6]])) # this automatically converts the array to 32 bits (no need to specify float32 as above!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., 88.,  7.],\n",
       "        [ 2.,  3.,  6.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code: demonstrate torch.from_numpy()\n",
    "X2_np = np.array([[1., 4., 7.], [2., 3., 6]])\n",
    "X2 = torch.from_numpy(X2_np)  # X2_np and X2 share the same data in memory\n",
    "X2_np[0, 1] = 88\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., -99.,   7.],\n",
       "        [  2., -99.,   6.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, 1] = -99\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 7.],\n",
       "        [2., 0., 6.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.relu_() # applies the ReLU activation function in place by replacing all negative values with 0s!\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch tensors really resemble NumPy arrays. In fact, they have over 200 common functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__getattr__, abs, absolute, acos, acosh, add, all, allclose, amax, amin, angle, any, arange, arccos, arccosh, arcsin, arcsinh, arctan, arctan2, arctanh, argmax, argmin, argsort, argwhere, asarray, asin, asinh, atan, atan2, atanh, atleast_1d, atleast_2d, atleast_3d, bincount, bitwise_and, bitwise_left_shift, bitwise_not, bitwise_or, bitwise_right_shift, bitwise_xor, broadcast_shapes, broadcast_to, can_cast, ceil, clip, column_stack, concat, concatenate, conj, copysign, corrcoef, cos, cosh, count_nonzero, cov, cross, cumprod, cumsum, deg2rad, diag, diagflat, diagonal, diff, divide, dot, dsplit, dstack, dtype, einsum, empty, empty_like, equal, exp, exp2, expm1, eye, finfo, fix, flip, fliplr, flipud, float_power, floor, floor_divide, fmax, fmin, fmod, frexp, from_dlpack, frombuffer, full, full_like, gcd, gradient, greater, greater_equal, heaviside, histogram, histogramdd, hsplit, hstack, hypot, i0, iinfo, imag, inner, isclose, isfinite, isin, isinf, isnan, isneginf, isposinf, isreal, kron, lcm, ldexp, less, less_equal, linspace, load, log, log10, log1p, log2, logaddexp, logaddexp2, logical_and, logical_not, logical_or, logical_xor, logspace, matmul, max, maximum, mean, median, meshgrid, min, minimum, moveaxis, multiply, nan_to_num, nanmean, nanmedian, nanquantile, nansum, negative, nextafter, nonzero, not_equal, ones, ones_like, outer, positive, pow, prod, promote_types, put, quantile, rad2deg, ravel, real, reciprocal, remainder, reshape, result_type, roll, rot90, round, row_stack, save, searchsorted, select, set_printoptions, sign, signbit, sin, sinc, sinh, sort, split, sqrt, square, squeeze, stack, std, subtract, sum, swapaxes, take, tan, tanh, tensordot, tile, trace, transpose, trapezoid, trapz, tril, tril_indices, triu, triu_indices, true_divide, trunc, typename, unique, unravel_index, vander, var, vdot, vsplit, vstack, where, zeros, zeros_like'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code: list functions that appear both in NumPy and PyTorch\n",
    "functions = lambda mod: set(f for f in dir(mod) if callable(getattr(mod, f)))\n",
    "\", \".join(sorted(functions(torch) & functions(np)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n",
    "M = M.to(device)\n",
    "M.device # <- to see which device a tensor lives on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device=device) # <- to create a tensor directly on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14., 32.],\n",
       "        [32., 77.]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = M @ M.T  # run some operations on the GPU\n",
    "R # <- result also lives on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the result also immediately lives on the GPU means we can perform multiple operations on the GPU without having to transfer data back and forth between the CPU and the GPU. This is crucial in deep learning because data transfer between devices can often become a performance bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4 ms ± 238 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "181 μs ± 7.75 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand((1000, 1000))  # on the CPU\n",
    "M @ M.T  # warmup\n",
    "%timeit M @ M.T\n",
    "\n",
    "M = M.to(device)\n",
    "M @ M.T  # warmup\n",
    "%timeit M @ M.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a simple function, $f(x) = x^2$.\n",
    "Calculus tells us that the derivative of this function is $f'(x)=2x$. Let's evaluate $f(5)$ and the derivative $f'(5)$ using autograd. We expect to find $f(5)=5^2=25$ and $f'(5)=2*5=10$. Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "f = x ** 2\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f also carries a grad_fn attribute which represents the operation that created this tensor (**, power, hence the name PowBackward0), and which tells PyTorch how to backpropagate the gradients through this particular operation. This grad_fn attribute is how PyTorch keeps track of the computation graph.\n",
    "\n",
    "Next, we call f.backward(): this backpropagates the gradients through the computation graph, starting with f, and all the way back to the leaf nodes (just x in this case).\n",
    "\n",
    "Lastly, we can just read the x tensor’s grad attribute, which was computed during backprop: this gives us the derivative of f with regard to x. Ta-da!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do a gradient descent step, you must temporarily disable gradient tracking since you don’t want to track the gradient descent step itself in the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "with torch.no_grad(): # <-\n",
    "    x -= learning_rate * x.grad  # gradient descent step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could have used this code for the gradient descent step (but using `no_grad()` is more common for this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_detached = x.detach()\n",
    "x_detached -= learning_rate * x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_detached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since x_detached and x share the same memory, modifying x_detached also modifies x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before you repeat the whole process (forward pass + backward pass + gradient descent step), it’s essential to zero out the gradients of every model parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put everything together to get our training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "for iteration in range(100):\n",
    "    f = x ** 2  # forward pass\n",
    "    f.backward()  # backward pass\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad  # gradient descent step\n",
    "    x.grad.zero_()  # reset the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `x` gets pushed towards 0, since that's the value that minimizes $f(x) = x^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0185e-09, requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Linear Regression\n",
    "## Linear Regression Using Tensors & Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "housing: Bunch = fetch_california_housing() # type: ignore\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data,\n",
    "    housing.target,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full,\n",
    "    y_train_full,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 20640\n",
      "\n",
      ":Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      ":Attribute Information:\n",
      "    - MedInc        median income in block group\n",
      "    - HouseAge      median house age in block group\n",
      "    - AveRooms      average number of rooms per household\n",
      "    - AveBedrms     average number of bedrooms per household\n",
      "    - Population    block group population\n",
      "    - AveOccup      average number of household members\n",
      "    - Latitude      block group latitude\n",
      "    - Longitude     block group longitude\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. rubric:: References\n",
      "\n",
      "- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "  Statistics and Probability Letters, 33:291-297, 1997.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(housing.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_valid = torch.FloatTensor(X_valid)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "means = X_train.mean(dim=0, keepdims=True)\n",
    "stds = X_train.std(dim=0, keepdims=True)\n",
    "# ... normalising it manually instead of using StandardScaler()...\n",
    "X_train = (X_train - means) / stds\n",
    "X_valid = (X_valid - means) / stds\n",
    "X_test = (X_test - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11610, 8])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3870, 8])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5160, 8])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch expects the targets to have one row per sample, so let's reshape the targets to be column vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.442, 1.687, 1.621, ..., 0.68 , 0.613, 1.97 ], shape=(11610,))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.FloatTensor(y_train).view(-1, 1)\n",
    "y_valid = torch.FloatTensor(y_valid).view(-1, 1)\n",
    "y_test = torch.FloatTensor(y_test).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11610, 8])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "n_features = X_train.shape[1]  # there are 8 input features\n",
    "w = torch.randn((n_features, 1), requires_grad=True) # initialised randomly for the 8 input features (this will be updated by GD), also a column vector.\n",
    "# (for this example, apparently, we could've initialised these to 0 too, but for NN later we need to initialise them randomly to break symmetry)\n",
    "b = torch.tensor(0., requires_grad=True) # initialised to 0, a single scalar (this will be updated by GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3367],\n",
       "        [ 0.1288],\n",
       "        [ 0.2345],\n",
       "        [ 0.2303],\n",
       "        [-1.1229],\n",
       "        [-0.1863],\n",
       "        [ 2.2082],\n",
       "        [-0.6380]], requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in the next section, we will build an almost identical model using PyTorch's high-level API. Its results will be slightly different because it will use a different parameter initialization method: it will use a uniform random distribution from $-\\frac{1}{2\\sqrt 2}$ to $+\\frac{1}{2\\sqrt 2}$ to initialize both the weights and the bias term. If you want to get exactly the same result here as in the next section, you can uncomment and run the initialization code in the following cell, instead of the code in the previous cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "# n_features = X_train.shape[1]  # there are 8 input features\n",
    "# r = 2 ** -1.5  # this is equal to 1 / 2√2\n",
    "# w = torch.empty(n_features, 1).uniform_(-r, r)\n",
    "# b = torch.empty(1).uniform_(-r, r)\n",
    "# w.requires_grad_(True)\n",
    "# b.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 16.158456802368164\n",
      "Epoch 2/20, Loss: 4.879363536834717\n",
      "Epoch 3/20, Loss: 2.2552270889282227\n",
      "Epoch 4/20, Loss: 1.3307628631591797\n",
      "Epoch 5/20, Loss: 0.9680694937705994\n",
      "Epoch 6/20, Loss: 0.8142679929733276\n",
      "Epoch 7/20, Loss: 0.7417048811912537\n",
      "Epoch 8/20, Loss: 0.7020703554153442\n",
      "Epoch 9/20, Loss: 0.6765920519828796\n",
      "Epoch 10/20, Loss: 0.6577966213226318\n",
      "Epoch 11/20, Loss: 0.6426153182983398\n",
      "Epoch 12/20, Loss: 0.6297224760055542\n",
      "Epoch 13/20, Loss: 0.6184942722320557\n",
      "Epoch 14/20, Loss: 0.608596920967102\n",
      "Epoch 15/20, Loss: 0.5998217463493347\n",
      "Epoch 16/20, Loss: 0.5920187830924988\n",
      "Epoch 17/20, Loss: 0.5850692391395569\n",
      "Epoch 18/20, Loss: 0.578873336315155\n",
      "Epoch 19/20, Loss: 0.5733454823493958\n",
      "Epoch 20/20, Loss: 0.5684100985527039\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.4\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = X_train @ w + b\n",
    "    loss = ((y_pred - y_train) ** 2).mean() # this is the root mean square error (RMSE) without the root (because minimising this also minimises the squared version of this)\n",
    "    loss.backward() # this is the back propagation step (compute the reverse graph). This populates the computed gradients into the variables involved in the computation (w and b)\n",
    "    with torch.no_grad():\n",
    "        b -= learning_rate * b.grad # the *actual* gradient descent step!\n",
    "        w -= learning_rate * w.grad # ^\n",
    "        b.grad.zero_() # resetting them in place to enable the next loop\n",
    "        w.grad.zero_() # resetting them in place to enable the next loop\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9142],\n",
       "        [ 0.1941],\n",
       "        [-0.3354],\n",
       "        [ 0.3345],\n",
       "        [ 0.0282],\n",
       "        [-0.0922],\n",
       "        [-0.3295],\n",
       "        [-0.3016]], requires_grad=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w # now this should be the 8 values satisfying the linear regression equation that minimise the loss (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0821, requires_grad=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b # and this should be the final constant that does the same ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]  # pretend these are new instances\n",
    "with torch.no_grad():\n",
    "    y_pred = X_new @ w + b  # use the trained parameters to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1578, -0.2867, -0.4955, -0.1662, -0.0295,  0.3890,  0.1937,  0.2870],\n",
       "        [-0.7125,  0.1088, -0.1633,  0.2016,  0.1284, -0.1182, -0.2372,  0.0622],\n",
       "        [-0.2156,  1.8491, -0.5798,  0.1853, -0.1043, -0.6769,  1.0089, -1.4271]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8916],\n",
       "        [1.6480],\n",
       "        [2.6577]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Using PyTorch's High-Level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)  # to get reproducible results\n",
    "model = nn.Linear(in_features=n_features, out_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.3117], requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias # bias vector with one bias term per neuron (we have a single neuron since out_features=1 -> single bias term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2703,  0.2935, -0.0828,  0.3248, -0.0775,  0.0713, -0.1721,  0.2076]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight # weight matrix with one row per neuron and one column per input dimension (we have a single neuron since out_features=1 -> single row for that neuron)\n",
    "# (both bias and weight are initialised randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2703,  0.2935, -0.0828,  0.3248, -0.0775,  0.0713, -0.1721,  0.2076]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3117], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4718],\n",
       "        [ 0.1131]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4420],\n",
       "        [1.6870]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # choose an optimiser (to update the model parameters) - the optim.SGD one can be used for SGD, mini-batch GD, or batch GD\n",
    "mse = nn.MSELoss() # choose a loss function (here Mean Square Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bgd(model, optimizer, criterion, X_train, y_train, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        y_pred = model(X_train)\n",
    "        loss = criterion(y_pred, y_train) # In PyTorch, the loss function object is commonly referred to as the criterion, to distinguish it from the loss value itself (which is computed at each training iteration using the criterion).\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 4.3378496170043945\n",
      "Epoch 2/20, Loss: 0.780293345451355\n",
      "Epoch 3/20, Loss: 0.6253840327262878\n",
      "Epoch 4/20, Loss: 0.6060433983802795\n",
      "Epoch 5/20, Loss: 0.595629870891571\n",
      "Epoch 6/20, Loss: 0.5873566269874573\n",
      "Epoch 7/20, Loss: 0.5802990198135376\n",
      "Epoch 8/20, Loss: 0.5741382241249084\n",
      "Epoch 9/20, Loss: 0.5687100887298584\n",
      "Epoch 10/20, Loss: 0.5639079213142395\n",
      "Epoch 11/20, Loss: 0.5596510767936707\n",
      "Epoch 12/20, Loss: 0.5558737516403198\n",
      "Epoch 13/20, Loss: 0.5525193810462952\n",
      "Epoch 14/20, Loss: 0.5495391488075256\n",
      "Epoch 15/20, Loss: 0.5468899011611938\n",
      "Epoch 16/20, Loss: 0.5445338487625122\n",
      "Epoch 17/20, Loss: 0.5424376130104065\n",
      "Epoch 18/20, Loss: 0.5405715703964233\n",
      "Epoch 19/20, Loss: 0.5389096736907959\n",
      "Epoch 20/20, Loss: 0.5374288558959961\n"
     ]
    }
   ],
   "source": [
    "train_bgd(model, optimizer, mse, X_train, y_train, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8061],\n",
       "        [1.7116],\n",
       "        [2.6973]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]  # pretend these are new instances\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_new)  # use the trained model to make predictions\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Regression MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 5.045480251312256\n",
      "Epoch 2/20, Loss: 2.0523128509521484\n",
      "Epoch 3/20, Loss: 1.00398850440979\n",
      "Epoch 4/20, Loss: 0.8570139408111572\n",
      "Epoch 5/20, Loss: 0.7740675210952759\n",
      "Epoch 6/20, Loss: 0.7225848436355591\n",
      "Epoch 7/20, Loss: 0.689372718334198\n",
      "Epoch 8/20, Loss: 0.6669032573699951\n",
      "Epoch 9/20, Loss: 0.650773823261261\n",
      "Epoch 10/20, Loss: 0.6383934020996094\n",
      "Epoch 11/20, Loss: 0.6281994581222534\n",
      "Epoch 12/20, Loss: 0.6193399429321289\n",
      "Epoch 13/20, Loss: 0.6113173365592957\n",
      "Epoch 14/20, Loss: 0.6038705706596375\n",
      "Epoch 15/20, Loss: 0.5968307852745056\n",
      "Epoch 16/20, Loss: 0.5901118516921997\n",
      "Epoch 17/20, Loss: 0.583646833896637\n",
      "Epoch 18/20, Loss: 0.5774063467979431\n",
      "Epoch 19/20, Loss: 0.5713554620742798\n",
      "Epoch 20/20, Loss: 0.565444827079773\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "mse = nn.MSELoss()\n",
    "train_bgd(model, optimizer, mse, X_train, y_train, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8370],\n",
       "        [1.2752],\n",
       "        [2.7642]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4770],\n",
       "        [0.4580],\n",
       "        [5.0000]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Mini-Batch Gradient Descent using DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    # pin_memory=True,  (use this when using a CUDA device!)\n",
    "    #num_workers=8  (youc an speed up training by pre-fetching the next batches on the CPU while the GPU is still working on the current batch. <- To be tweaked based on your hardware with concerns for windows.)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pin_memory=True` will allocate the data in page-locked memory which guarantees a fixed physical memory location in the CPU RAM, and therefore allows direct memory access (DMA) transfers to the GPU, eliminating an extra copy operation that would otherwise be needed.\n",
    "\n",
    " While this could use more CPU RAM since the memory cannot be swapped out to disk, it typically results in significantly faster data transfers and thus faster training. When transferring a tensor to the GPU using its `to()` method, you may also set `non_blocking=True` to avoid blocking the CPU during the data transfer (this only works if `pin_memory=True`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – build the model just like earlier\n",
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features, 50), nn.ReLU(),\n",
    "    nn.Linear(50, 40), nn.ReLU(),\n",
    "    nn.Linear(40, 1)\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# extra code – build the optimizer and loss function, as earlier\n",
    "learning_rate = 0.02\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, n_epochs):\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        mean_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {mean_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.5900\n",
      "Epoch 2/20, Loss: 0.4046\n",
      "Epoch 3/20, Loss: 0.3801\n",
      "Epoch 4/20, Loss: 0.3629\n",
      "Epoch 5/20, Loss: 0.3529\n",
      "Epoch 6/20, Loss: 0.3519\n",
      "Epoch 7/20, Loss: 0.3408\n",
      "Epoch 8/20, Loss: 0.3425\n",
      "Epoch 9/20, Loss: 0.3405\n",
      "Epoch 10/20, Loss: 0.3373\n",
      "Epoch 11/20, Loss: 0.3298\n",
      "Epoch 12/20, Loss: 0.3263\n",
      "Epoch 13/20, Loss: 0.3239\n",
      "Epoch 14/20, Loss: 0.3224\n",
      "Epoch 15/20, Loss: 0.3187\n",
      "Epoch 16/20, Loss: 0.3149\n",
      "Epoch 17/20, Loss: 0.3121\n",
      "Epoch 18/20, Loss: 0.3106\n",
      "Epoch 19/20, Loss: 0.3088\n",
      "Epoch 20/20, Loss: 0.3088\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, mse, train_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader: DataLoader, metric_fn, aggregate_fn=torch.mean):\n",
    "    \"\"\"\n",
    "    :param model:           The model we want to evaluate\n",
    "    :param data_loader:     The dataset that we want to evaluate the model on\n",
    "    :param metric_fn:       Function to compute the metric for a given batch, like *Mean Square Error*\n",
    "    :param aggregate_fn:    Function to aggregate the batch metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metrics = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            metric = metric_fn(y_pred, y_batch)\n",
    "            metrics.append(metric)\n",
    "    return aggregate_fn(torch.stack(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4057, device='cuda:0')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
    "valid_mse = evaluate(model, valid_loader, mse)\n",
    "valid_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5667, device='cuda:0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmse(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Root mean square error\n",
    "\n",
    "    > **RMSE is easier to interpret than MSE** because it is in the **same units as the target variable**, making it more intuitive for understanding the typical size of prediction errors.\n",
    "    \"\"\"\n",
    "    return ((y_pred - y_true) ** 2).mean().sqrt() # we had MSE above and this is doing the final missing SQRT operation\n",
    "\n",
    "evaluate(model, valid_loader, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6369, device='cuda:0')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_mse.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6369, device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, valid_loader, mse,\n",
    "         aggregate_fn=lambda metrics: torch.sqrt(torch.mean(metrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "def evaluate_tm(model, data_loader: DataLoader, metric: torchmetrics.Metric):\n",
    "    \"\"\"\n",
    "    Evaluate using the torchmetrics library to aggregate the data with a\n",
    "    standard way of aggregating the error mathematically over all the loops\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric.reset()  # reset the metric at the beginning\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            metric.update(y_pred, y_batch)  # update it at each iteration\n",
    "\n",
    "    return metric.compute()  # compute the final result at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6370, device='cuda:0')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "evaluate_tm(model, valid_loader, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(model, optimizer, criterion, metric: torchmetrics.Metric, train_loader: DataLoader, valid_loader: DataLoader, n_epochs: int):\n",
    "\n",
    "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        metric.reset()\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            model.train()\n",
    "\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "\n",
    "            total_loss += loss.item() # track the total loss while training...\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            metric.update(y_pred, y_batch)\n",
    "\n",
    "        mean_loss = total_loss / len(train_loader)\n",
    "\n",
    "        history[\"train_losses\"].append(mean_loss)\n",
    "        # this is what we just computed live.\n",
    "        history[\"train_metrics\"].append(metric.compute().item())\n",
    "        # then do an evaluation against the validation set (metric is reused but zeroe'd out)\n",
    "        history[\"valid_metrics\"].append(evaluate_tm(model, valid_loader, metric).item())\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, \"\n",
    "              f\"train loss: {history['train_losses'][-1]:.4f}, \"\n",
    "              f\"train metric: {history['train_metrics'][-1]:.4f}, \"\n",
    "              f\"valid metric: {history['valid_metrics'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, train loss: 0.7826, train metric: 0.8847, valid metric: 0.6690\n",
      "Epoch 2/20, train loss: 0.4362, train metric: 0.6605, valid metric: 0.6099\n",
      "Epoch 3/20, train loss: 0.3930, train metric: 0.6269, valid metric: 0.6145\n",
      "Epoch 4/20, train loss: 0.3759, train metric: 0.6132, valid metric: 0.5963\n",
      "Epoch 5/20, train loss: 0.3649, train metric: 0.6040, valid metric: 0.5911\n",
      "Epoch 6/20, train loss: 0.3598, train metric: 0.5999, valid metric: 0.5965\n",
      "Epoch 7/20, train loss: 0.3530, train metric: 0.5941, valid metric: 0.6061\n",
      "Epoch 8/20, train loss: 0.3495, train metric: 0.5911, valid metric: 0.6043\n",
      "Epoch 9/20, train loss: 0.3455, train metric: 0.5877, valid metric: 0.5723\n",
      "Epoch 10/20, train loss: 0.3416, train metric: 0.5846, valid metric: 0.6043\n",
      "Epoch 11/20, train loss: 0.3401, train metric: 0.5831, valid metric: 0.5882\n",
      "Epoch 12/20, train loss: 0.3362, train metric: 0.5799, valid metric: 0.5738\n",
      "Epoch 13/20, train loss: 0.3352, train metric: 0.5788, valid metric: 0.5873\n",
      "Epoch 14/20, train loss: 0.3310, train metric: 0.5754, valid metric: 0.5884\n",
      "Epoch 15/20, train loss: 0.3290, train metric: 0.5736, valid metric: 0.5608\n",
      "Epoch 16/20, train loss: 0.3272, train metric: 0.5721, valid metric: 0.5755\n",
      "Epoch 17/20, train loss: 0.3264, train metric: 0.5714, valid metric: 0.5837\n",
      "Epoch 18/20, train loss: 0.3238, train metric: 0.5691, valid metric: 0.5662\n",
      "Epoch 19/20, train loss: 0.3208, train metric: 0.5663, valid metric: 0.5557\n",
      "Epoch 20/20, train loss: 0.3190, train metric: 0.5649, valid metric: 0.5618\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "learning_rate = 0.01\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features, 50), nn.ReLU(),\n",
    "    nn.Linear(50, 40), nn.ReLU(),\n",
    "    nn.Linear(40, 30), nn.ReLU(),\n",
    "    nn.Linear(30, 1)\n",
    ")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
    "mse = nn.MSELoss()\n",
    "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "history = train2(\n",
    "    model,\n",
    "    optimizer,\n",
    "    mse, # cheaper to compute\n",
    "    rmse, # easier to interpret aka \"the average error is $500 and not $250,000^2\"\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    n_epochs\n",
    ")\n",
    "\n",
    "# Since we compute the training metric\n",
    "plt.plot(\n",
    "    np.arange(n_epochs) + 0.5,\n",
    "    history[\"train_metrics\"],\n",
    "    \".--\",\n",
    "    label=\"Training\"\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(n_epochs) + 1.0,\n",
    "    history[\"valid_metrics\"],\n",
    "    \".-\",\n",
    "    label=\"Validation\"\n",
    ")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid()\n",
    "plt.title(\"Learning curves\")\n",
    "plt.axis([0.5, 20, 0.4, 1.0])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Nonsequential Models Using Custom Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeep(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "\n",
    "        # all the layers we need must be created in the constructor (after base __init__)\n",
    "        self.deep_stack = nn.Sequential(\n",
    "            nn.Linear(n_features, 50), nn.ReLU(),\n",
    "            nn.Linear(50, 40), nn.ReLU(),\n",
    "            nn.Linear(40, 30), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(30 + n_features, 1)\n",
    "        # + n_features because we decided we will feed the last layers (wide part) in raw to the output layer\n",
    "        # So we have to arrange that manually in the forward() pass (below)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        deep_output = self.deep_stack(X)\n",
    "        wide_and_deep = torch.concat([X, deep_output], dim=1)\n",
    "        return self.output_layer(wide_and_deep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = WideAndDeep(n_features).to(device)\n",
    "learning_rate = 0.002  # the model changed, so did the optimal learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, train loss: 1.7802, train metric: 1.3344, valid metric: 0.8690\n",
      "Epoch 2/20, train loss: 0.6201, train metric: 0.7875, valid metric: 0.9492\n",
      "Epoch 3/20, train loss: 0.5900, train metric: 0.7682, valid metric: 0.7331\n",
      "Epoch 4/20, train loss: 0.5607, train metric: 0.7488, valid metric: 0.7771\n",
      "Epoch 5/20, train loss: 0.5408, train metric: 0.7353, valid metric: 0.7967\n",
      "Epoch 6/20, train loss: 0.5244, train metric: 0.7241, valid metric: 0.7098\n",
      "Epoch 7/20, train loss: 0.5070, train metric: 0.7119, valid metric: 0.7419\n",
      "Epoch 8/20, train loss: 0.4941, train metric: 0.7030, valid metric: 0.6750\n",
      "Epoch 9/20, train loss: 0.4798, train metric: 0.6928, valid metric: 0.6762\n",
      "Epoch 10/20, train loss: 0.4657, train metric: 0.6825, valid metric: 0.6678\n",
      "Epoch 11/20, train loss: 0.4538, train metric: 0.6736, valid metric: 0.6617\n",
      "Epoch 12/20, train loss: 0.4441, train metric: 0.6665, valid metric: 0.6651\n",
      "Epoch 13/20, train loss: 0.4328, train metric: 0.6580, valid metric: 0.6803\n",
      "Epoch 14/20, train loss: 0.4232, train metric: 0.6506, valid metric: 0.6288\n",
      "Epoch 15/20, train loss: 0.4139, train metric: 0.6434, valid metric: 0.6202\n",
      "Epoch 16/20, train loss: 0.4063, train metric: 0.6374, valid metric: 0.6216\n",
      "Epoch 17/20, train loss: 0.3991, train metric: 0.6317, valid metric: 0.6129\n",
      "Epoch 18/20, train loss: 0.3937, train metric: 0.6274, valid metric: 0.6031\n",
      "Epoch 19/20, train loss: 0.3879, train metric: 0.6228, valid metric: 0.6091\n",
      "Epoch 20/20, train loss: 0.3834, train metric: 0.6193, valid metric: 0.5957\n"
     ]
    }
   ],
   "source": [
    "# extra code: train the model, exactly our previous models\n",
    "optimizer = torch.optim.SGD(\n",
    "    # the parameters are the parameters that we are trying to optimise by Gradient Descent, e.g. (w + b) in Linear Regression\n",
    "    # in neural networks the w is a matrix rather than just a single array.\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    momentum=0\n",
    ")\n",
    "mse = nn.MSELoss()\n",
    "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "history = train2(\n",
    "    model,\n",
    "    optimizer,\n",
    "    mse,\n",
    "    rmse,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepV2(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.deep_stack = nn.Sequential(\n",
    "            nn.Linear(n_features - 2, 50), nn.ReLU(),\n",
    "            nn.Linear(50, 40), nn.ReLU(),\n",
    "            nn.Linear(40, 30), nn.ReLU(),\n",
    "        )\n",
    "        self.output_layer = nn.Linear(30 + 5, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X_wide = X[:, :5] # first 5 features\n",
    "        X_deep = X[:, 2:] # features 3 through 8\n",
    "        deep_output = self.deep_stack(X_deep)\n",
    "        wide_and_deep = torch.concat([X_wide, deep_output], dim=1)\n",
    "        return self.output_layer(wide_and_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = WideAndDeepV2(n_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, train loss: 1.8482, train metric: 1.3598, valid metric: 0.9100\n",
      "Epoch 2/20, train loss: 0.6282, train metric: 0.7927, valid metric: 0.8028\n",
      "Epoch 3/20, train loss: 0.5763, train metric: 0.7591, valid metric: 0.7567\n",
      "Epoch 4/20, train loss: 0.5413, train metric: 0.7356, valid metric: 0.7290\n",
      "Epoch 5/20, train loss: 0.5099, train metric: 0.7142, valid metric: 0.7011\n",
      "Epoch 6/20, train loss: 0.4841, train metric: 0.6958, valid metric: 0.6816\n",
      "Epoch 7/20, train loss: 0.4656, train metric: 0.6824, valid metric: 0.6670\n",
      "Epoch 8/20, train loss: 0.4526, train metric: 0.6728, valid metric: 0.6576\n",
      "Epoch 9/20, train loss: 0.4438, train metric: 0.6662, valid metric: 0.6539\n",
      "Epoch 10/20, train loss: 0.4380, train metric: 0.6618, valid metric: 0.6498\n",
      "Epoch 11/20, train loss: 0.4326, train metric: 0.6577, valid metric: 0.6470\n",
      "Epoch 12/20, train loss: 0.4284, train metric: 0.6546, valid metric: 0.6447\n",
      "Epoch 13/20, train loss: 0.4253, train metric: 0.6521, valid metric: 0.6452\n",
      "Epoch 14/20, train loss: 0.4216, train metric: 0.6494, valid metric: 0.6468\n",
      "Epoch 15/20, train loss: 0.4190, train metric: 0.6473, valid metric: 0.6484\n",
      "Epoch 16/20, train loss: 0.4169, train metric: 0.6458, valid metric: 0.6452\n",
      "Epoch 17/20, train loss: 0.4144, train metric: 0.6438, valid metric: 0.6459\n",
      "Epoch 18/20, train loss: 0.4118, train metric: 0.6417, valid metric: 0.6470\n",
      "Epoch 19/20, train loss: 0.4101, train metric: 0.6404, valid metric: 0.6475\n",
      "Epoch 20/20, train loss: 0.4082, train metric: 0.6389, valid metric: 0.6493\n"
     ]
    }
   ],
   "source": [
    "# extra code: train the model, exactly our previous models\n",
    "learning_rate = 0.002\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
    "mse = nn.MSELoss()\n",
    "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "history = train2(model, optimizer, mse, rmse, train_loader, valid_loader,\n",
    "                 n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models with Multiple Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepV3(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.deep_stack = nn.Sequential(\n",
    "            nn.Linear(n_features - 2, 50), nn.ReLU(),\n",
    "            nn.Linear(50, 40), nn.ReLU(),\n",
    "            nn.Linear(40, 30), nn.ReLU(),\n",
    "        )\n",
    "        self.output_layer = nn.Linear(30 + 5, 1)\n",
    "\n",
    "    def forward(self, X_wide, X_deep): # takes multiple inputs!\n",
    "        deep_output = self.deep_stack(X_deep)\n",
    "        wide_and_deep = torch.concat([X_wide, deep_output], dim=1)\n",
    "        return self.output_layer(wide_and_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"mermaid-dabd55c1-89b3-4025-a2c0-1390976a8faf\"></div>\n",
       "        <script type=\"module\">\n",
       "            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.1.0/+esm'\n",
       "            const graphDefinition = '%%{\\ninit: {\\n        \"theme\": \"base\",\\n        \"themeVariables\": {\\n        \"primaryColor\": \"#494f5c\",\\n        \"primaryTextColor\": \"#fff\",\\n        \"primaryBorderColor\": \"#5f6573\",\\n        \"lineColor\": \"#9a9ca1\",\\n        \"secondaryColor\": \"#006100\",\\n        \"tertiaryColor\": \"#fff\"\\n        }\\n    }\\n}%%\\n\\n\\ngraph BT\\n    style D fill:BlanchedAlmond,color:black\\n    style E fill:BlanchedAlmond,color:black\\n    style B fill:DarkOrchid\\n    style F fill:LightSkyBlue,color:black\\n    style A fill:transparent,stroke:transparent\\n    style C fill:transparent,stroke:transparent\\n\\n    A(Input wide) ----> B[Concat]\\n    C(Input deep) --> D[Hidden 1]\\n    D --> E[Hidden 2]\\n    E --> B\\n    B --> F[Output Layer]\\n    F --> G((( )))\\n\\n';\n",
       "            const element = document.querySelector('.mermaid-dabd55c1-89b3-4025-a2c0-1390976a8faf');\n",
       "            const { svg } = await mermaid.render('graphDiv-dabd55c1-89b3-4025-a2c0-1390976a8faf', graphDefinition);\n",
       "            element.innerHTML = svg;\n",
       "        </script>\n",
       "        "
      ],
      "text/plain": [
       "<mermaid.mermaid.Mermaid at 0x7a9f01dd2090>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mermaid_diagram(\"\"\"\n",
    "graph BT\n",
    "    style D fill:BlanchedAlmond,color:black\n",
    "    style E fill:BlanchedAlmond,color:black\n",
    "    style B fill:DarkOrchid\n",
    "    style F fill:LightSkyBlue,color:black\n",
    "    style A fill:transparent,stroke:transparent\n",
    "    style C fill:transparent,stroke:transparent\n",
    "\n",
    "    A(Input wide) ----> B[Concat]\n",
    "    C(Input deep) --> D[Hidden 1]\n",
    "    D --> E[Hidden 2]\n",
    "    E --> B\n",
    "    B --> F[Output Layer]\n",
    "    F --> G((( )))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_data_wd = TensorDataset(X_train[:, :5], X_train[:, 2:], y_train)\n",
    "train_loader_wd = DataLoader(train_data_wd, batch_size=32, shuffle=True)\n",
    "valid_data_wd = TensorDataset(X_valid[:, :5], X_valid[:, 2:], y_valid)\n",
    "valid_loader_wd = DataLoader(valid_data_wd, batch_size=32)\n",
    "test_data_wd = TensorDataset(X_test[:, :5], X_test[:, 2:], y_test)\n",
    "test_loader_wd = DataLoader(test_data_wd, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the train and eval loops to deal with a Dataloader returning not two but three tensors at every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, train loss: 0.8366, train metric: 0.9148, valid metric: 0.6892\n",
      "Epoch 2/20, train loss: 0.4627, train metric: 0.6803, valid metric: 0.6455\n",
      "Epoch 3/20, train loss: 0.4319, train metric: 0.6572, valid metric: 0.6374\n",
      "Epoch 4/20, train loss: 0.4259, train metric: 0.6525, valid metric: 0.6512\n",
      "Epoch 5/20, train loss: 0.4120, train metric: 0.6420, valid metric: 0.6305\n",
      "Epoch 6/20, train loss: 0.4040, train metric: 0.6356, valid metric: 0.6287\n",
      "Epoch 7/20, train loss: 0.4005, train metric: 0.6330, valid metric: 0.6252\n",
      "Epoch 8/20, train loss: 0.3976, train metric: 0.6306, valid metric: 0.6158\n",
      "Epoch 9/20, train loss: 0.3883, train metric: 0.6230, valid metric: 0.7407\n",
      "Epoch 10/20, train loss: 0.3866, train metric: 0.6218, valid metric: 0.6063\n",
      "Epoch 11/20, train loss: 0.3752, train metric: 0.6125, valid metric: 0.5974\n",
      "Epoch 12/20, train loss: 0.3704, train metric: 0.6087, valid metric: 0.5887\n",
      "Epoch 13/20, train loss: 0.3677, train metric: 0.6063, valid metric: 0.5981\n",
      "Epoch 14/20, train loss: 0.3605, train metric: 0.6005, valid metric: 0.5841\n",
      "Epoch 15/20, train loss: 0.3596, train metric: 0.5997, valid metric: 0.6706\n",
      "Epoch 16/20, train loss: 0.3646, train metric: 0.6039, valid metric: 0.5747\n",
      "Epoch 17/20, train loss: 0.3534, train metric: 0.5945, valid metric: 0.5691\n",
      "Epoch 18/20, train loss: 0.3463, train metric: 0.5885, valid metric: 0.5677\n",
      "Epoch 19/20, train loss: 0.3439, train metric: 0.5864, valid metric: 0.5626\n",
      "Epoch 20/20, train loss: 0.3412, train metric: 0.5842, valid metric: 0.5716\n"
     ]
    }
   ],
   "source": [
    "def evaluate_multi_in(model, data_loader, metric):\n",
    "    model.eval()\n",
    "    metric.reset()  # reset the metric at the beginning\n",
    "    with torch.no_grad():\n",
    "        for X_batch_wide, X_batch_deep, y_batch in data_loader:\n",
    "            X_batch_wide = X_batch_wide.to(device)\n",
    "            X_batch_deep = X_batch_deep.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(X_batch_wide, X_batch_deep)\n",
    "            metric.update(y_pred, y_batch)  # update it at each iteration\n",
    "    return metric.compute()  # compute the final result at the end\n",
    "\n",
    "def train_multi_in(model, optimizer, criterion, metric, train_loader,\n",
    "                   valid_loader, n_epochs):\n",
    "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        metric.reset()\n",
    "        for *X_batch_inputs, y_batch in train_loader:\n",
    "            model.train()\n",
    "            X_batch_inputs = [X.to(device) for X in X_batch_inputs]\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(*X_batch_inputs)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            metric.update(y_pred, y_batch)\n",
    "        mean_loss = total_loss / len(train_loader)\n",
    "        history[\"train_losses\"].append(mean_loss)\n",
    "        history[\"train_metrics\"].append(metric.compute().item())\n",
    "        history[\"valid_metrics\"].append(\n",
    "            evaluate_multi_in(model, valid_loader, metric).item())\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, \"\n",
    "              f\"train loss: {history['train_losses'][-1]:.4f}, \"\n",
    "              f\"train metric: {history['train_metrics'][-1]:.4f}, \"\n",
    "              f\"valid metric: {history['valid_metrics'][-1]:.4f}\")\n",
    "    return history\n",
    "\n",
    "torch.manual_seed(42)\n",
    "learning_rate = 0.01\n",
    "model = WideAndDeepV3(n_features).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
    "mse = nn.MSELoss()\n",
    "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "history = train_multi_in(model, optimizer, mse, rmse, train_loader_wd,\n",
    "                         valid_loader_wd, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your model has many inputs, it’s easy to make a mistake and mix up the order of the inputs, which can lead to hard-to-debug issues. To avoid this, it can be a good idea to name each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_wide, X_deep, y):\n",
    "        self.X_wide = X_wide\n",
    "        self.X_deep = X_deep\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_dict = {\"X_wide\": self.X_wide[idx], \"X_deep\": self.X_deep[idx]}\n",
    "        return input_dict, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_data_named = WideAndDeepDataset(\n",
    "    X_wide=X_train[:, :5],\n",
    "    X_deep=X_train[:, 2:],\n",
    "    y=y_train\n",
    ")\n",
    "train_loader_named = DataLoader(train_data_named, batch_size=32, shuffle=True)\n",
    "\n",
    "valid_data_named = WideAndDeepDataset(\n",
    "    X_wide=X_valid[:, :5],\n",
    "    X_deep=X_valid[:, 2:],\n",
    "    y=y_valid\n",
    ")\n",
    "valid_loader_named = DataLoader(valid_data_named, batch_size=32)\n",
    "test_data_named = WideAndDeepDataset(\n",
    "    X_wide=X_test[:, :5],\n",
    "    X_deep=X_test[:, 2:],\n",
    "    y=y_test\n",
    ")\n",
    "test_loader_named = DataLoader(test_data_named, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, train loss: 0.8366, train metric: 0.9148, valid metric: 0.6892\n",
      "Epoch 2/20, train loss: 0.4627, train metric: 0.6803, valid metric: 0.6455\n",
      "Epoch 3/20, train loss: 0.4319, train metric: 0.6572, valid metric: 0.6374\n",
      "Epoch 4/20, train loss: 0.4259, train metric: 0.6525, valid metric: 0.6512\n",
      "Epoch 5/20, train loss: 0.4120, train metric: 0.6420, valid metric: 0.6305\n",
      "Epoch 6/20, train loss: 0.4040, train metric: 0.6356, valid metric: 0.6287\n",
      "Epoch 7/20, train loss: 0.4005, train metric: 0.6330, valid metric: 0.6252\n",
      "Epoch 8/20, train loss: 0.3976, train metric: 0.6306, valid metric: 0.6158\n",
      "Epoch 9/20, train loss: 0.3883, train metric: 0.6230, valid metric: 0.7407\n",
      "Epoch 10/20, train loss: 0.3866, train metric: 0.6218, valid metric: 0.6063\n",
      "Epoch 11/20, train loss: 0.3752, train metric: 0.6125, valid metric: 0.5974\n",
      "Epoch 12/20, train loss: 0.3704, train metric: 0.6087, valid metric: 0.5887\n",
      "Epoch 13/20, train loss: 0.3677, train metric: 0.6063, valid metric: 0.5981\n",
      "Epoch 14/20, train loss: 0.3605, train metric: 0.6005, valid metric: 0.5841\n",
      "Epoch 15/20, train loss: 0.3596, train metric: 0.5997, valid metric: 0.6706\n",
      "Epoch 16/20, train loss: 0.3646, train metric: 0.6039, valid metric: 0.5747\n",
      "Epoch 17/20, train loss: 0.3534, train metric: 0.5945, valid metric: 0.5691\n",
      "Epoch 18/20, train loss: 0.3463, train metric: 0.5885, valid metric: 0.5677\n",
      "Epoch 19/20, train loss: 0.3439, train metric: 0.5864, valid metric: 0.5626\n",
      "Epoch 20/20, train loss: 0.3412, train metric: 0.5842, valid metric: 0.5716\n"
     ]
    }
   ],
   "source": [
    "def evaluate_named(model, data_loader: DataLoader[WideAndDeepDataset], metric):\n",
    "    model.eval()\n",
    "    metric.reset()  # reset the metric at the beginning\n",
    "    with torch.no_grad():\n",
    "        for inputs, y_batch in data_loader:\n",
    "            inputs = {name: X.to(device) for name, X in inputs.items()}\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(X_wide=inputs[\"X_wide\"], X_deep=inputs[\"X_deep\"])\n",
    "            metric.update(y_pred, y_batch)\n",
    "    return metric.compute()  # compute the final result at the end\n",
    "\n",
    "def train_named(model, optimizer, criterion, metric, train_loader,\n",
    "                   valid_loader, n_epochs):\n",
    "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        metric.reset()\n",
    "        for inputs, y_batch in train_loader:\n",
    "            model.train()\n",
    "            inputs = {name: X.to(device) for name, X in inputs.items()}\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(**inputs)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            metric.update(y_pred, y_batch)\n",
    "        mean_loss = total_loss / len(train_loader)\n",
    "        history[\"train_losses\"].append(mean_loss)\n",
    "        history[\"train_metrics\"].append(metric.compute().item())\n",
    "        history[\"valid_metrics\"].append(\n",
    "            evaluate_named(model, valid_loader, metric).item())\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, \"\n",
    "              f\"train loss: {history['train_losses'][-1]:.4f}, \"\n",
    "              f\"train metric: {history['train_metrics'][-1]:.4f}, \"\n",
    "              f\"valid metric: {history['valid_metrics'][-1]:.4f}\")\n",
    "    return history\n",
    "\n",
    "torch.manual_seed(42)\n",
    "learning_rate = 0.01\n",
    "model = WideAndDeepV3(n_features).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
    "mse = nn.MSELoss()\n",
    "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "history = train_named(model, optimizer, mse, rmse, train_loader_named,\n",
    "                      valid_loader_named, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models with Multiple Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"mermaid-c85251fe-1f4d-4753-b0c0-ee61f9292176\"></div>\n",
       "        <script type=\"module\">\n",
       "            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.1.0/+esm'\n",
       "            const graphDefinition = '%%{\\ninit: {\\n        \"theme\": \"base\",\\n        \"themeVariables\": {\\n        \"primaryColor\": \"#494f5c\",\\n        \"primaryTextColor\": \"#fff\",\\n        \"primaryBorderColor\": \"#5f6573\",\\n        \"lineColor\": \"#9a9ca1\",\\n        \"secondaryColor\": \"transparent\",\\n        \"tertiaryColor\": \"#fff\"\\n        }\\n    }\\n}%%\\n\\n\\ngraph BT\\n    style D fill:BlanchedAlmond,color:black\\n    style E fill:BlanchedAlmond,color:black\\n    style B fill:DarkOrchid\\n    style F fill:LightSkyBlue,color:black\\n    style A fill:transparent,stroke:transparent\\n    style C fill:transparent,stroke:transparent\\n    style H fill:LightSkyBlue,color:black\\n\\n    A(Input wide) ----> B[Concat]\\n    C(Input deep) --> D[Hidden 1]\\n    subgraph Deep Stack\\n        D --> E[Hidden 2]\\n    end\\n    E --> B\\n    B --> F[Output Layer]\\n    F --> G((( )))\\n    E ---> H[Auxiliary Output Layer]\\n    H -- Just for regularisation... --> I((( )))\\n\\n';\n",
       "            const element = document.querySelector('.mermaid-c85251fe-1f4d-4753-b0c0-ee61f9292176');\n",
       "            const { svg } = await mermaid.render('graphDiv-c85251fe-1f4d-4753-b0c0-ee61f9292176', graphDefinition);\n",
       "            element.innerHTML = svg;\n",
       "        </script>\n",
       "        "
      ],
      "text/plain": [
       "<mermaid.mermaid.Mermaid at 0x7a9f01c5aa20>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mermaid_diagram(\"\"\"\n",
    "graph BT\n",
    "    style D fill:BlanchedAlmond,color:black\n",
    "    style E fill:BlanchedAlmond,color:black\n",
    "    style B fill:DarkOrchid\n",
    "    style F fill:LightSkyBlue,color:black\n",
    "    style A fill:transparent,stroke:transparent\n",
    "    style C fill:transparent,stroke:transparent\n",
    "    style H fill:LightSkyBlue,color:black\n",
    "\n",
    "    A(Input wide) ----> B[Concat]\n",
    "    C(Input deep) --> D[Hidden 1]\n",
    "    subgraph Deep Stack\n",
    "        D --> E[Hidden 2]\n",
    "    end\n",
    "    E --> B\n",
    "    B --> F[Output Layer]\n",
    "    F --> G((( )))\n",
    "    E ---> H[Auxiliary Output Layer]\n",
    "    H -- Just for regularisation... --> I((( )))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepV4(nn.Module):\n",
    "    \"\"\"\n",
    "    See image above above definition\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.deep_stack = nn.Sequential(\n",
    "            nn.Linear(n_features - 2, 50), nn.ReLU(),\n",
    "            nn.Linear(50, 40), nn.ReLU(),\n",
    "            nn.Linear(40, 30), nn.ReLU(),\n",
    "        )\n",
    "        self.output_layer = nn.Linear(30 + 5, 1)\n",
    "        self.aux_output_layer = nn.Linear(30, 1)\n",
    "\n",
    "    def forward(self, X_wide, X_deep):\n",
    "\n",
    "        deep_output = self.deep_stack(X_deep)\n",
    "        wide_and_deep = torch.concat([X_wide, deep_output], dim=1)\n",
    "        main_output = self.output_layer(wide_and_deep)\n",
    "        aux_output = self.aux_output_layer(deep_output)\n",
    "        return main_output, aux_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a loss for each output, we must combine them into a single loss that will be minimized by gradient descent. **In general, this final loss is just a weighted sum of all the output losses.** \n",
    "\n",
    "In this example, we use a higher weight for the main loss (`0.8`), because that’s what we care about the most, and a lower weight for the auxiliary loss (`0.2`). \n",
    "\n",
    "👉 This ratio is a regularization hyperparameter that you can tune.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, train loss: 1.0693, train metric: 0.9506, valid metric: 0.7085\n",
      "Epoch 2/20, train loss: 0.5817, train metric: 0.6946, valid metric: 0.6607\n",
      "Epoch 3/20, train loss: 0.5009, train metric: 0.6581, valid metric: 0.6425\n",
      "Epoch 4/20, train loss: 0.4690, train metric: 0.6497, valid metric: 0.6654\n",
      "Epoch 5/20, train loss: 0.4503, train metric: 0.6420, valid metric: 0.6338\n",
      "Epoch 6/20, train loss: 0.4385, train metric: 0.6371, valid metric: 0.6560\n",
      "Epoch 7/20, train loss: 0.4314, train metric: 0.6329, valid metric: 0.6193\n",
      "Epoch 8/20, train loss: 0.4248, train metric: 0.6301, valid metric: 0.6166\n",
      "Epoch 9/20, train loss: 0.4115, train metric: 0.6201, valid metric: 0.6454\n",
      "Epoch 10/20, train loss: 0.4087, train metric: 0.6200, valid metric: 0.5940\n",
      "Epoch 11/20, train loss: 0.4075, train metric: 0.6198, valid metric: 0.5954\n",
      "Epoch 12/20, train loss: 0.3914, train metric: 0.6077, valid metric: 0.6072\n",
      "Epoch 13/20, train loss: 0.3847, train metric: 0.6033, valid metric: 0.5817\n",
      "Epoch 14/20, train loss: 0.3850, train metric: 0.6049, valid metric: 0.6070\n",
      "Epoch 15/20, train loss: 0.3746, train metric: 0.5966, valid metric: 0.5728\n",
      "Epoch 16/20, train loss: 0.3694, train metric: 0.5929, valid metric: 0.5997\n",
      "Epoch 17/20, train loss: 0.3678, train metric: 0.5927, valid metric: 0.6005\n",
      "Epoch 18/20, train loss: 0.3612, train metric: 0.5872, valid metric: 0.5681\n",
      "Epoch 19/20, train loss: 0.3595, train metric: 0.5862, valid metric: 0.5716\n",
      "Epoch 20/20, train loss: 0.3557, train metric: 0.5831, valid metric: 0.6065\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "\n",
    "def evaluate_multi_out(model, data_loader, metric):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    with torch.no_grad():\n",
    "        for inputs, y_batch in data_loader:\n",
    "            inputs = {name: X.to(device) for name, X in inputs.items()}\n",
    "            y_batch = y_batch.to(device) # We ignore the auxiliary output in the evaluation function! (It was only there for regularization during training)\n",
    "            y_pred, _ = model(**inputs)\n",
    "            metric.update(y_pred, y_batch)\n",
    "    return metric.compute()\n",
    "\n",
    "def train_multi_out(model, optimizer, criterion, metric, train_loader,\n",
    "                   valid_loader, n_epochs):\n",
    "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        metric.reset()\n",
    "        for inputs, y_batch in train_loader:\n",
    "            model.train()\n",
    "            inputs = {name: X.to(device) for name, X in inputs.items()}\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred, y_pred_aux = model(**inputs) # <- this now produces the y_pred AND that auxiliary output\n",
    "            main_loss = criterion(y_pred, y_batch)\n",
    "            aux_loss = criterion(y_pred_aux, y_batch) # <- in this case it's just using y_batch itself to minimise the loss on, but it could be anything I guess...\n",
    "\n",
    "            # The ratio v\n",
    "            loss = 0.8 * main_loss + 0.2 * aux_loss # <- And then it's used to compute a total loss that we will minimise like this...\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            metric.update(y_pred, y_batch)\n",
    "        mean_loss = total_loss / len(train_loader)\n",
    "        history[\"train_losses\"].append(mean_loss)\n",
    "        history[\"train_metrics\"].append(metric.compute().item())\n",
    "        history[\"valid_metrics\"].append(\n",
    "            evaluate_multi_out(model, valid_loader, metric).item())\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, \"\n",
    "              f\"train loss: {history['train_losses'][-1]:.4f}, \"\n",
    "              f\"train metric: {history['train_metrics'][-1]:.4f}, \"\n",
    "              f\"valid metric: {history['valid_metrics'][-1]:.4f}\")\n",
    "    return history\n",
    "\n",
    "torch.manual_seed(42)\n",
    "learning_rate = 0.01\n",
    "model = WideAndDeepV4(n_features).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
    "mse = nn.MSELoss()\n",
    "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "history = train_multi_out(model, optimizer, mse, rmse, train_loader_named,\n",
    "                          valid_loader_named, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Image Classifier with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TorchVision to Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:00<00:00, 93.1MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 4.24MB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:00<00:00, 51.0MB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 48.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms.v2 as T\n",
    "\n",
    "toTensor = T.Compose([T.ToImage(), T.ToDtype(torch.float32, scale=True)])\n",
    "\n",
    "train_and_valid_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"datasets\",\n",
    "    train=True, # load the train set\n",
    "    download=True, # download the dataset if it cannot be found locally\n",
    "    transform=toTensor, # we set this to use our custom preprocessing pipeline to transform PIL images to pytorch tensors\n",
    "    #target_transform=None (if we wanted to apply a custom preprocessing pipeline to the targets)\n",
    ")\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"datasets\",\n",
    "    train=False, # load the test set\n",
    "    download=True, # download the dataset if it cannot be found locally\n",
    "    transform=toTensor # we set this to use our custom preprocessing pipeline to transform PIL images to pytorch tensors\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_data, valid_data = torch.utils.data.random_split(\n",
    "    train_and_valid_data,\n",
    "    [55_000, 5_000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7a9f01c14980>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=32)\n",
    "test_loader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry is a tuple (image, target):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample, y_sample = train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image has a shape \\[channels, rows, columns\\]. Grayscale images like in Fashion MNIST have a single channel (while RGB images have 3, and other types of images, such as satellite images, may have many more). Fashion images are grayscale and 28x28 pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample # <- targets are integers from 0 to 9, which can be interpreted as the corresponding strings using the classes array below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ankle boot'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_valid_data.classes[y_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ℹ️ PyTorch expects the channel dimension to be first, while many other libraries, such as Matplotlib, PIL, TensorFlow, OpenCV, or Scikit-Image, expect it to be last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"mermaid-d68baf11-1914-4afe-abd6-3cfd83068c87\"></div>\n",
       "        <script type=\"module\">\n",
       "            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.1.0/+esm'\n",
       "            const graphDefinition = '%%{\\ninit: {\\n        \"theme\": \"base\",\\n        \"themeVariables\": {\\n        \"primaryColor\": \"#494f5c\",\\n        \"primaryTextColor\": \"#fff\",\\n        \"primaryBorderColor\": \"#5f6573\",\\n        \"lineColor\": \"#9a9ca1\",\\n        \"secondaryColor\": \"transparent\",\\n        \"tertiaryColor\": \"#fff\"\\n        }\\n    }\\n}%%\\n\\n\\ngraph BT\\n    style A fill:transparent,stroke:transparent\\n    style B fill:BlanchedAlmond,color:black\\n    style C fill:BlanchedAlmond,color:black\\n    style D fill:LightSkyBlue,color:black\\n\\n    A[Input] --> B[Hidden 1]\\n    B --> C[Hidden 2]\\n    C --> D[Output Layer]\\n    D --> E((( )))\\n\\n';\n",
       "            const element = document.querySelector('.mermaid-d68baf11-1914-4afe-abd6-3cfd83068c87');\n",
       "            const { svg } = await mermaid.render('graphDiv-d68baf11-1914-4afe-abd6-3cfd83068c87', graphDefinition);\n",
       "            element.innerHTML = svg;\n",
       "        </script>\n",
       "        "
      ],
      "text/plain": [
       "<mermaid.mermaid.Mermaid at 0x7a9f030de870>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mermaid_diagram(\"\"\"\n",
    "graph BT\n",
    "    style A fill:transparent,stroke:transparent\n",
    "    style B fill:BlanchedAlmond,color:black\n",
    "    style C fill:BlanchedAlmond,color:black\n",
    "    style D fill:LightSkyBlue,color:black\n",
    "\n",
    "    A[Input] --> B[Hidden 1]\n",
    "    B --> C[Hidden 2]\n",
    "    C --> D[Output Layer]\n",
    "    D --> E((( )))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden1: int, n_hidden2: int, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(), # reshapes the input samples to a single dimension (needed for the nn.Linear layers!)\n",
    "            # For example, a batch of 32 Fashion MNIST images has a shape of [32, 1, 28, 28],\n",
    "            # but after going through the nn.Flatten layer, it ends up with a shape of [32, 784]\n",
    "            # (since 28 × 28 = 784).\n",
    "            nn.Linear(n_inputs, n_hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden1, n_hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden2, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mlp(X)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = ImageClassifier(\n",
    "    n_inputs=1 * 28 * 28,\n",
    "    n_hidden1=300,\n",
    "    n_hidden2=100,\n",
    "    n_classes=10\n",
    ").to(device)\n",
    "\n",
    "# Use crossEntropyLoss since it's a multiclass classification task\n",
    "xentropy = nn.CrossEntropyLoss(\n",
    "    #label_smoothing=0.05\n",
    "    # ^\n",
    "    #  If you wish to apply label smoothing during training, just set the\n",
    "    # label_smoothing hyperparameter of the nn.CrossEntropyLoss to the amount\n",
    "    # of smoothing you wish, between 0 and 1 (e.g., 0.05).\n",
    "\n",
    "    # weight=torch.tensor([0.2205, 0.2835, 0.4961])\n",
    "    # ^\n",
    "    # The Fashion MNIST dataset is balanced, meaning it has the same number of instances of each class.\n",
    "    # When dealing with an unbalanced dataset, you should generally give more weight to the rare classes\n",
    "    # and less weight to the frequent ones, or else your model will be biased toward the more frequent classes.\n",
    "    # You can do this by setting the weight argument of the nn.CrossEntropyLoss.\n",
    "    # For example, if there are three classes with 900, 700, and 400 instances,\n",
    "    # respectively (i.e., 2000 instances in total), then the respective weights\n",
    "    # should be 2000/900, 2000/700, and 2000/400. It’s preferable to normalize these\n",
    "    # weights to ensure they add up to 1.\n",
    ")\n",
    "\n",
    "# nn.CrossEntropyLoss accepts either class indices as targets (such as here) or class probabilities (such as one-hot vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, train loss: 0.1820, train metric: 0.9298, valid metric: 0.8928\n",
      "Epoch 2/20, train loss: 0.1786, train metric: 0.9319, valid metric: 0.8870\n",
      "Epoch 3/20, train loss: 0.1740, train metric: 0.9335, valid metric: 0.8816\n",
      "Epoch 4/20, train loss: 0.1696, train metric: 0.9357, valid metric: 0.8874\n",
      "Epoch 5/20, train loss: 0.1659, train metric: 0.9361, valid metric: 0.8884\n",
      "Epoch 6/20, train loss: 0.1596, train metric: 0.9398, valid metric: 0.8828\n",
      "Epoch 7/20, train loss: 0.1567, train metric: 0.9394, valid metric: 0.8820\n",
      "Epoch 8/20, train loss: 0.1524, train metric: 0.9410, valid metric: 0.8826\n",
      "Epoch 9/20, train loss: 0.1480, train metric: 0.9431, valid metric: 0.8660\n",
      "Epoch 10/20, train loss: 0.1485, train metric: 0.9428, valid metric: 0.8932\n",
      "Epoch 11/20, train loss: 0.1408, train metric: 0.9460, valid metric: 0.8872\n",
      "Epoch 12/20, train loss: 0.1396, train metric: 0.9463, valid metric: 0.8852\n",
      "Epoch 13/20, train loss: 0.1354, train metric: 0.9483, valid metric: 0.8864\n",
      "Epoch 14/20, train loss: 0.1313, train metric: 0.9495, valid metric: 0.8842\n",
      "Epoch 15/20, train loss: 0.1263, train metric: 0.9523, valid metric: 0.8928\n",
      "Epoch 16/20, train loss: 0.1273, train metric: 0.9515, valid metric: 0.8912\n",
      "Epoch 17/20, train loss: 0.1240, train metric: 0.9523, valid metric: 0.8900\n",
      "Epoch 18/20, train loss: 0.1223, train metric: 0.9526, valid metric: 0.8892\n",
      "Epoch 19/20, train loss: 0.1175, train metric: 0.9548, valid metric: 0.8744\n",
      "Epoch 20/20, train loss: 0.1151, train metric: 0.9554, valid metric: 0.8874\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "_ = train2(\n",
    "    model,\n",
    "    optimizer,\n",
    "    xentropy,\n",
    "    accuracy, # <- metric\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 4, 2], device='cuda:0')"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "X_new, y_new = next(iter(valid_loader))\n",
    "X_new = X_new[:3].to(device)\n",
    "with torch.no_grad():\n",
    "    y_pred_logits = model(X_new)\n",
    "y_pred = y_pred_logits.argmax(dim=1)  # index of the largest logit\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sneaker', 'Coat', 'Pullover']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[train_and_valid_data.classes[index] for index in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether the model made the correct predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 4, 2])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All correct! 😃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the predicted class probabilities we need to manually compute the softmax as we didn't include a softmax in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0330, 0.0000, 0.9670, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.8550, 0.0000, 0.1160, 0.0000, 0.0290, 0.0000, 0.0000,\n",
       "         0.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "y_proba = F.softmax(y_pred_logits, dim=1)\n",
    "if device == \"mps\":\n",
    "    y_proba = y_proba.cpu() # to cpu\n",
    "y_proba.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can often be useful to get the model’s top k predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.9670, 0.0330, 0.0000, 0.0000],\n",
       "        [0.8550, 0.1160, 0.0290, 0.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_top4_values, y_top4_indices = torch.topk(y_pred_logits, k=4, dim=1)\n",
    "y_top4_probas = F.softmax(y_top4_values, dim=1)\n",
    "if device == \"mps\":\n",
    "    y_top4_probas = y_top4_probas.cpu()\n",
    "y_top4_probas.round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 9, 5, 1],\n",
       "        [4, 2, 6, 8],\n",
       "        [2, 4, 6, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_top4_indices # these indices with the above probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sneaker', 'Ankle boot', 'Sandal', 'Trouser']\n",
      "['Coat', 'Pullover', 'Shirt', 'Bag']\n",
      "['Pullover', 'Coat', 'Shirt', 'T-shirt/top']\n"
     ]
    }
   ],
   "source": [
    "for preds in y_top4_indices: # corresponding to these classes...\n",
    "    print([train_and_valid_data.classes[index] for index in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266610"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([param.numel() for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    n_hidden = trial.suggest_int(\"n_hidden\", 20, 300)\n",
    "    model = ImageClassifier(n_inputs=1 * 28 * 28, n_hidden1=n_hidden,\n",
    "                            n_hidden2=n_hidden, n_classes=10).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    xentropy = nn.CrossEntropyLoss()\n",
    "    accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "    accuracy = accuracy.to(device)\n",
    "    history = train2(model, optimizer, xentropy, accuracy, train_loader,\n",
    "                     valid_loader, n_epochs=10)\n",
    "    validation_accuracy = max(history[\"valid_metrics\"])\n",
    "    return validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-07 07:20:53,727] A new study created in memory with name: no-name-27fd1323-5c92-43f9-8071-fa800aebc54b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, train loss: 2.2769, train metric: 0.1471, valid metric: 0.1860\n",
      "Epoch 2/10, train loss: 2.2093, train metric: 0.2794, valid metric: 0.3500\n",
      "Epoch 3/10, train loss: 2.1164, train metric: 0.4110, valid metric: 0.4554\n",
      "Epoch 4/10, train loss: 1.9776, train metric: 0.5137, valid metric: 0.5562\n",
      "Epoch 5/10, train loss: 1.7867, train metric: 0.5826, valid metric: 0.6026\n",
      "Epoch 6/10, train loss: 1.5775, train metric: 0.6184, valid metric: 0.6228\n",
      "Epoch 7/10, train loss: 1.3978, train metric: 0.6288, valid metric: 0.6326\n",
      "Epoch 8/10, train loss: 1.2605, train metric: 0.6360, valid metric: 0.6372\n",
      "Epoch 9/10, train loss: 1.1572, train metric: 0.6468, valid metric: 0.6424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-07 07:23:25,327] Trial 0 finished with value: 0.6435999870300293 and parameters: {'learning_rate': 0.00031489116479568613, 'n_hidden': 287}. Best is trial 0 with value: 0.6435999870300293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, train loss: 1.0782, train metric: 0.6537, valid metric: 0.6436\n",
      "Epoch 1/10, train loss: 1.1459, train metric: 0.6229, valid metric: 0.7338\n",
      "Epoch 2/10, train loss: 0.6108, train metric: 0.7841, valid metric: 0.7992\n",
      "Epoch 3/10, train loss: 0.5203, train metric: 0.8169, valid metric: 0.8094\n",
      "Epoch 4/10, train loss: 0.4810, train metric: 0.8302, valid metric: 0.8310\n",
      "Epoch 5/10, train loss: 0.4557, train metric: 0.8404, valid metric: 0.8352\n",
      "Epoch 6/10, train loss: 0.4387, train metric: 0.8460, valid metric: 0.8442\n",
      "Epoch 7/10, train loss: 0.4240, train metric: 0.8512, valid metric: 0.8408\n",
      "Epoch 8/10, train loss: 0.4123, train metric: 0.8566, valid metric: 0.8514\n",
      "Epoch 9/10, train loss: 0.3998, train metric: 0.8601, valid metric: 0.8532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-07 07:25:57,218] Trial 1 finished with value: 0.8547999858856201 and parameters: {'learning_rate': 0.008471801418819975, 'n_hidden': 188}. Best is trial 1 with value: 0.8547999858856201.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, train loss: 0.3897, train metric: 0.8638, valid metric: 0.8548\n",
      "Epoch 1/10, train loss: 2.3069, train metric: 0.1144, valid metric: 0.1082\n",
      "Epoch 2/10, train loss: 2.2993, train metric: 0.1231, valid metric: 0.1294\n",
      "Epoch 3/10, train loss: 2.2914, train metric: 0.1606, valid metric: 0.1710\n",
      "Epoch 4/10, train loss: 2.2836, train metric: 0.1839, valid metric: 0.1840\n",
      "Epoch 5/10, train loss: 2.2762, train metric: 0.1891, valid metric: 0.1856\n",
      "Epoch 6/10, train loss: 2.2692, train metric: 0.1910, valid metric: 0.1898\n",
      "Epoch 7/10, train loss: 2.2623, train metric: 0.1933, valid metric: 0.1932\n",
      "Epoch 8/10, train loss: 2.2554, train metric: 0.2000, valid metric: 0.2022\n",
      "Epoch 9/10, train loss: 2.2485, train metric: 0.2122, valid metric: 0.2160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-07 07:28:28,531] Trial 2 finished with value: 0.23340000212192535 and parameters: {'learning_rate': 4.207988669606632e-05, 'n_hidden': 63}. Best is trial 1 with value: 0.8547999858856201.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, train loss: 2.2414, train metric: 0.2299, valid metric: 0.2334\n",
      "Epoch 1/10, train loss: 2.3035, train metric: 0.1373, valid metric: 0.1526\n",
      "Epoch 2/10, train loss: 2.3005, train metric: 0.1569, valid metric: 0.1724\n",
      "Epoch 3/10, train loss: 2.2975, train metric: 0.1755, valid metric: 0.1896\n",
      "Epoch 4/10, train loss: 2.2945, train metric: 0.1941, valid metric: 0.2132\n",
      "Epoch 5/10, train loss: 2.2914, train metric: 0.2105, valid metric: 0.2288\n",
      "Epoch 6/10, train loss: 2.2884, train metric: 0.2261, valid metric: 0.2418\n",
      "Epoch 7/10, train loss: 2.2853, train metric: 0.2419, valid metric: 0.2580\n",
      "Epoch 8/10, train loss: 2.2823, train metric: 0.2581, valid metric: 0.2742\n",
      "Epoch 9/10, train loss: 2.2792, train metric: 0.2736, valid metric: 0.2918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-07 07:30:58,822] Trial 3 finished with value: 0.30959999561309814 and parameters: {'learning_rate': 1.7073967431528103e-05, 'n_hidden': 263}. Best is trial 1 with value: 0.8547999858856201.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, train loss: 2.2761, train metric: 0.2897, valid metric: 0.3096\n",
      "Epoch 1/10, train loss: 1.8379, train metric: 0.4869, valid metric: 0.6208\n",
      "Epoch 2/10, train loss: 0.9751, train metric: 0.6666, valid metric: 0.6978\n",
      "Epoch 3/10, train loss: 0.7608, train metric: 0.7253, valid metric: 0.7416\n",
      "Epoch 4/10, train loss: 0.6704, train metric: 0.7639, valid metric: 0.7720\n",
      "Epoch 5/10, train loss: 0.6108, train metric: 0.7913, valid metric: 0.7906\n",
      "Epoch 6/10, train loss: 0.5687, train metric: 0.8053, valid metric: 0.8050\n",
      "Epoch 7/10, train loss: 0.5386, train metric: 0.8164, valid metric: 0.8082\n",
      "Epoch 8/10, train loss: 0.5158, train metric: 0.8243, valid metric: 0.8214\n",
      "Epoch 9/10, train loss: 0.4988, train metric: 0.8279, valid metric: 0.8220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-07 07:33:29,943] Trial 4 finished with value: 0.8220000267028809 and parameters: {'learning_rate': 0.002537815508265664, 'n_hidden': 218}. Best is trial 1 with value: 0.8547999858856201.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, train loss: 0.4842, train metric: 0.8330, valid metric: 0.8092\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.008471801418819975, 'n_hidden': 188}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8547999858856201"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, train_loader, valid_loader):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    n_hidden = trial.suggest_int(\"n_hidden\", 20, 300)\n",
    "    model = ImageClassifier(n_inputs=1 * 28 * 28, n_hidden1=n_hidden,\n",
    "                            n_hidden2=n_hidden, n_classes=10).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    xentropy = nn.CrossEntropyLoss()\n",
    "    accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "    accuracy = accuracy.to(device)\n",
    "    best_validation_accuracy = 0.0\n",
    "    for epoch in range(n_epochs):\n",
    "        history = train2(model, optimizer, xentropy, accuracy, train_loader,\n",
    "                         valid_loader, n_epochs=1)\n",
    "        validation_accuracy = max(history[\"valid_metrics\"])\n",
    "        if validation_accuracy > best_validation_accuracy:\n",
    "            best_validation_accuracy = validation_accuracy\n",
    "        trial.report(validation_accuracy, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    return best_validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_with_data = lambda trial: objective(\n",
    "    trial, train_loader=train_loader, valid_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "objective_with_data = partial(objective, train_loader=train_loader,\n",
    "                              valid_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-07 07:33:29,993] A new study created in memory with name: no-name-38c4ab57-e010-4c31-b3e6-9cf84c20adce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, train loss: 2.2769, train metric: 0.1471, valid metric: 0.1860\n",
      "Epoch 1/1, train loss: 2.2093, train metric: 0.2794, valid metric: 0.3500\n",
      "Epoch 1/1, train loss: 2.1164, train metric: 0.4110, valid metric: 0.4554\n",
      "Epoch 1/1, train loss: 1.9776, train metric: 0.5137, valid metric: 0.5562\n",
      "Epoch 1/1, train loss: 1.7867, train metric: 0.5826, valid metric: 0.6026\n",
      "Epoch 1/1, train loss: 1.5775, train metric: 0.6184, valid metric: 0.6228\n",
      "Epoch 1/1, train loss: 1.3978, train metric: 0.6288, valid metric: 0.6326\n",
      "Epoch 1/1, train loss: 1.2605, train metric: 0.6360, valid metric: 0.6372\n",
      "Epoch 1/1, train loss: 1.1572, train metric: 0.6468, valid metric: 0.6424\n",
      "Epoch 1/1, train loss: 1.0782, train metric: 0.6537, valid metric: 0.6436\n",
      "Epoch 1/1, train loss: 1.0162, train metric: 0.6611, valid metric: 0.6530\n",
      "Epoch 1/1, train loss: 0.9665, train metric: 0.6689, valid metric: 0.6620\n",
      "Epoch 1/1, train loss: 0.9258, train metric: 0.6761, valid metric: 0.6700\n",
      "Epoch 1/1, train loss: 0.8919, train metric: 0.6835, valid metric: 0.6782\n",
      "Epoch 1/1, train loss: 0.8629, train metric: 0.6897, valid metric: 0.6848\n",
      "Epoch 1/1, train loss: 0.8381, train metric: 0.6954, valid metric: 0.6876\n",
      "Epoch 1/1, train loss: 0.8166, train metric: 0.7009, valid metric: 0.6932\n",
      "Epoch 1/1, train loss: 0.7974, train metric: 0.7073, valid metric: 0.6964\n",
      "Epoch 1/1, train loss: 0.7802, train metric: 0.7119, valid metric: 0.7090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-07 07:38:38,280] Trial 0 finished with value: 0.7089999914169312 and parameters: {'learning_rate': 0.00031489116479568613, 'n_hidden': 287}. Best is trial 0 with value: 0.7089999914169312.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, train loss: 0.7647, train metric: 0.7196, valid metric: 0.7082\n",
      "Epoch 1/1, train loss: 1.1485, train metric: 0.6157, valid metric: 0.7332\n",
      "Epoch 1/1, train loss: 0.6133, train metric: 0.7864, valid metric: 0.8082\n",
      "Epoch 1/1, train loss: 0.5200, train metric: 0.8179, valid metric: 0.8136\n",
      "Epoch 1/1, train loss: 0.4783, train metric: 0.8311, valid metric: 0.8232\n",
      "Epoch 1/1, train loss: 0.4533, train metric: 0.8402, valid metric: 0.8020\n",
      "Epoch 1/1, train loss: 0.4357, train metric: 0.8465, valid metric: 0.8446\n",
      "Epoch 1/1, train loss: 0.4211, train metric: 0.8510, valid metric: 0.8276\n",
      "Epoch 1/1, train loss: 0.4083, train metric: 0.8562, valid metric: 0.8398\n",
      "Epoch 1/1, train loss: 0.3981, train metric: 0.8606, valid metric: 0.8532\n",
      "Epoch 1/1, train loss: 0.3881, train metric: 0.8640, valid metric: 0.8582\n",
      "Epoch 1/1, train loss: 0.3782, train metric: 0.8663, valid metric: 0.8532\n",
      "Epoch 1/1, train loss: 0.3699, train metric: 0.8693, valid metric: 0.8566\n",
      "Epoch 1/1, train loss: 0.3631, train metric: 0.8712, valid metric: 0.8574\n",
      "<<204 more lines>>\n",
      "Epoch 1/1, train loss: 0.2761, train metric: 0.8982, valid metric: 0.8792\n",
      "Epoch 1/1, train loss: 0.2684, train metric: 0.9003, valid metric: 0.8848\n",
      "Epoch 1/1, train loss: 0.2615, train metric: 0.9045, valid metric: 0.8772\n",
      "Epoch 1/1, train loss: 0.2560, train metric: 0.9049, valid metric: 0.8820\n",
      "Epoch 1/1, train loss: 0.2497, train metric: 0.9071, valid metric: 0.8832\n",
      "Epoch 1/1, train loss: 0.2434, train metric: 0.9097, valid metric: 0.8848\n",
      "Epoch 1/1, train loss: 0.2380, train metric: 0.9126, valid metric: 0.8832\n",
      "Epoch 1/1, train loss: 0.2331, train metric: 0.9145, valid metric: 0.8806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-07 08:32:19,547] Trial 16 finished with value: 0.8848000168800354 and parameters: {'learning_rate': 0.032666299131732864, 'n_hidden': 142}. Best is trial 12 with value: 0.8867999911308289.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, train loss: 0.2271, train metric: 0.9154, valid metric: 0.8800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-07 08:32:35,221] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, train loss: 1.5604, train metric: 0.5038, valid metric: 0.6536\n",
      "Epoch 1/1, train loss: 0.6189, train metric: 0.7751, valid metric: 0.8314\n",
      "Epoch 1/1, train loss: 0.4237, train metric: 0.8447, valid metric: 0.8492\n",
      "Epoch 1/1, train loss: 0.3814, train metric: 0.8605, valid metric: 0.8582\n",
      "Epoch 1/1, train loss: 0.3567, train metric: 0.8681, valid metric: 0.8622\n",
      "Epoch 1/1, train loss: 0.3375, train metric: 0.8744, valid metric: 0.8674\n",
      "Epoch 1/1, train loss: 0.3246, train metric: 0.8788, valid metric: 0.8480\n",
      "Epoch 1/1, train loss: 0.3123, train metric: 0.8849, valid metric: 0.8644\n",
      "Epoch 1/1, train loss: 0.3012, train metric: 0.8871, valid metric: 0.8734\n",
      "Epoch 1/1, train loss: 0.2915, train metric: 0.8916, valid metric: 0.8760\n",
      "Epoch 1/1, train loss: 0.2858, train metric: 0.8931, valid metric: 0.8758\n",
      "Epoch 1/1, train loss: 0.2778, train metric: 0.8955, valid metric: 0.8804\n",
      "Epoch 1/1, train loss: 0.2715, train metric: 0.8972, valid metric: 0.8616\n",
      "Epoch 1/1, train loss: 0.2642, train metric: 0.9011, valid metric: 0.8800\n",
      "Epoch 1/1, train loss: 0.2598, train metric: 0.9009, valid metric: 0.8782\n",
      "Epoch 1/1, train loss: 0.2561, train metric: 0.9033, valid metric: 0.8734\n",
      "Epoch 1/1, train loss: 0.2494, train metric: 0.9058, valid metric: 0.8788\n",
      "Epoch 1/1, train loss: 0.2474, train metric: 0.9069, valid metric: 0.8720\n",
      "Epoch 1/1, train loss: 0.2411, train metric: 0.9089, valid metric: 0.8750\n",
      "Epoch 1/1, train loss: 0.2375, train metric: 0.9103, valid metric: 0.8768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-07 08:37:45,329] Trial 18 finished with value: 0.8830000162124634 and parameters: {'learning_rate': 0.0954812841907134, 'n_hidden': 50}. Best is trial 12 with value: 0.8867999911308289.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, train loss: 0.2353, train metric: 0.9109, valid metric: 0.8830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-07 08:38:00,038] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, train loss: 0.7417, train metric: 0.7395, valid metric: 0.7640\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "pruner = optuna.pruners.MedianPruner()\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler,\n",
    "                            pruner=pruner)\n",
    "study.optimize(objective_with_data, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8867999911308289"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.08525846269447772, 'n_hidden': 116}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading a PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"my_fashion_mnist.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load(\"my_fashion_mnist.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.eval()\n",
    "y_pred_logits = loaded_model(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"my_fashion_mnist_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (mlp): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=300, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = ImageClassifier(n_inputs=1 * 28 * 28, n_hidden1=300, n_hidden2=100,\n",
    "                            n_classes=10)\n",
    "loaded_weights = torch.load(\"my_fashion_mnist_weights.pt\", weights_only=True)\n",
    "new_model.load_state_dict(loaded_weights)\n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = {\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"model_hyperparameters\": {\n",
    "        \"n_inputs\": 1 * 28 * 28,\n",
    "        \"n_hidden1\": 300,\n",
    "        \"n_hidden2\": 100,\n",
    "        \"n_classes\": 10,\n",
    "    }\n",
    "}\n",
    "torch.save(model_data, \"my_fashion_mnist_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (mlp): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=300, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data = torch.load(\"my_fashion_mnist_model.pt\", weights_only=True)\n",
    "new_model = ImageClassifier(**loaded_data[\"model_hyperparameters\"])\n",
    "new_model.load_state_dict(loaded_data[\"model_state_dict\"])\n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling and Optimizing a PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchscript_model = torch.jit.trace(model, X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchscript_model = torch.jit.script(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = torch.jit.optimize_for_inference(torchscript_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.save(\"my_fashion_mnist_torchscript.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_torchscript_model = torch.jit.load(\"my_fashion_mnist_torchscript.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -3.3324,  -0.7572,  -4.1671,  -1.9444,  -1.6955,   2.0123,  -3.2624,\n",
       "           8.6379,  -0.5685,   6.2972],\n",
       "        [ -0.1434,  -3.8626,  11.8279,  -0.3164,  17.3995, -13.1888,   7.8158,\n",
       "         -11.8338,  -1.4709,  -7.9412],\n",
       "        [  0.3996,  -2.5537,   7.5992,   0.1775,   6.9748,  -6.1942,   4.8270,\n",
       "          -5.8584,  -1.4327,  -4.3257]], device='cuda:0')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_logits = loaded_torchscript_model(X_new)\n",
    "y_pred_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1007 08:38:10.662000 360 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "    y_pred_logits = compiled_model(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises 1. to 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. PyTorch is similar to NumPy is many ways, but it offers some extra features. The main ones are:\n",
    "    * Auto-differentiation\n",
    "    * Support for hardware accelerators\n",
    "    * Includes optimizers and ready-to-use neural net components\n",
    "\n",
    "2. `torch.exp()` returns a copy of the input tensor while `torch.exp_()` modifies it in place. Similarly, `torch.relu()` returns a copy while `torch.relu_()` modifies in place.\n",
    "\n",
    "3. To create a new tensor on the GPU, you can use one of the following methods:\n",
    "     * Set the `device` argument when calling `torch.tensor()`, `torch.rand()`, or other functions that create new tensors. For example, `torch.randn(10, device=\"cuda\")` creates a new tensor on the CUDA GPU, with 10 random elements.\n",
    "     * Create a new tensor on the CPU, then transfer it to the GPU by using calling its `to()` method, for example `torch.randn(10).to(\"cuda\")`. However, it's more efficient to create the tensor directly on the GPU.\n",
    "     * The `*_like()` functions such as `ones_like()` and `zeros_like()` create a new tensor on the same device as another tensor. They also use the same data type.\n",
    "    * Lastly, if you execute an operation on a tensor that lives on the GPU, the result will generally be a new tensor on the GPU (unless you use an in-place operation such as `torch.exp_()`.\n",
    "\n",
    "4. Here are three ways to perform tensor computations without using autograd:\n",
    "    * Manipulate tensors created with `requires_grad=False` (which is the default).\n",
    "    * Run the computations inside a `with torch.no_grad():` block.\n",
    "    * Call the `detach()` method on the tensor you want to manipulate without autograd.\n",
    "\n",
    "5. Here's what happens in each case:\n",
    "    * The first code sample will work fine, it will _not_ cause a `RuntimeError`: indeed, the `cos()` method creates a new (non-leaf) tensor, then `exp_()` modifies it in place. During the backward pass, PyTorch is able to backpropagate through the `exp_()` operation because the derivative of exp(x) is exp(x), so PyTorch doesn't need to know what the input x was, it can just use the output of the forward pass (i.e., exp(x)) during the backward pass.\n",
    "    * If you replace `z = t.cos().exp_()` with `z = t.cos_().exp()` then you will get a `RuntimeError` on that line (\"_a leaf Variable that requires grad is being used in an in-place operation_\"). Indeed, `t` is a leaf tensor (since it was created directly by the user, not the result of any computation) and you cannot apply an in-place operation on a leaf tensor with `requires_grad=True`.\n",
    "    * If you replace `z = t.cos().exp_()` with `z = t.exp().cos_()`, then you will get a `RuntimeError` (\"_one of the variables needed for gradient computation has been modified by an inplace operation_\") during the backward pass. Indeed, the `exp()` operation relies on the fact that the derivative of exp(_t_) is exp(_t_), so it doesn't need to store the tensor `t` for the backward pass, instead it relies on the fact that it can just use the tensor returned by `t.exp()` (let's call it `e`). So far so good. But when we call the `cos_()` operation, it knows that it will need its input during the backward pass (since the derivative of cos(_e_) is –sin(_e_)), so it keeps a copy of its input tensor `e`. Next, it tries to modify the original `e` in-place, and in doing so it notices that this tensor is needed by another operation (`exp()`) for the backward pass, so it knows that something is fishy and it raises a `RuntimeError`.\n",
    "    * The second code example will fail during backpropogation, with a `RuntimeError`. It's a very similar error to the previous one: the `cos()` operation stores a reference to its input tensor `v`, since `v` will be needed during the backward pass. Indeed, the derivative of cos(_v_) is –sin(_v_), so we need to save _v_. Next, the `sin_()` operation creates a copy of its input `v` (since it's an in-place operation, it knows that it must create a copy, not just preserve a reference) then it proceeds to modify the original `v`, but this tensor is needed to compute the gradient of `cos(_v_)`, so PyTorch raises a `RuntimeError`.\n",
    "    * If you replace `w = v.cos() * v.sin_()` with `w = v.cos_() * v.sin()`, then there is no longer any `RuntimeError`. Indeed, the `cos_()` operation creates a copy of its input `v` so it can compute –sin(_v_) during the backward pass. The `sin()` operation is not in-place so in keeps a reference to its input `v`, not a copy. Backprop then runs just fine. However, there's a catch: by the time the `sin()` operation runs, its input `v` is no longer equal to 3.0, but instead it's equal to cos(3.0) since the `cos_()` modified `v` in place. As a result, `w = v.cos() * v.sin_()` does _not_ give the same result as `w = v.cos_() * v.sin()`. The former computes cos(3) * sin(3) while the latter computes cos(3) * sin(cos(3)). And of course the gradients change as well. That's why you should be very careful with in-place operations: they can make your code faster, sure, but they can also make it silently wrong.\n",
    "\n",
    "6. A `Linear(100, 200)` module has 200 neurons: one per output. Its `weight` tensor has a shape of [200, 100] and its `bias` parameter has a shape of [200]. It expects its inputs to have a shape of [..., 100], for example [32, 100], or [32, 64, 100]. It treats all dimensions independently, except for the last one. The output shape is identical to the input shape, except that the last dimension is replaced with 200. For example, if the input shape is [32, 64, 100], then the output shape is [32, 64, 200].\n",
    "\n",
    "7. The main steps of a PyTorch training loop are:\n",
    "    * Prepare a batch of samples from the training set. You can use a `DataLoader` for this.\n",
    "    * Optionally transfer these samples to the GPU (typically using `X_batch.to(device)` and `y_batch.to(device)`).\n",
    "    * Run the inputs through the model, for example `y_pred = model(X_batch)`.\n",
    "    * Compute the loss, for example `loss = criterion(y_pred, y_true)`.\n",
    "    * Backpropagate through the loss using `loss.backward()`.\n",
    "    * Perform an optimizer step: `optimizer.step()`.\n",
    "    * Zero out the gradients: `optimizer.zero_grad()` (alternatively, you can do this before the backward pass, which may be safer if the gradients are non-zero before the training loop starts).\n",
    "    * The whole training loop is often split into epochs, but this is optional.\n",
    "\n",
    "8. It is recommended to create the optimizer _after_ the model is moved to the GPU because most optimizers have some internal state, and this state is usually allocated on the same device as the model parameters.\n",
    "\n",
    "9. To speed up training when using a GPU, you should generally set the following `DataLoader` options:\n",
    "    * Set the data loader's `num_workers` argument to the number of processes you want to use for data loading and preprocessing. This will often speed up training by pre-fetching the next batches on the CPU while the GPU is still working on the current batch. The optimal number depends on your platform, hardware, and workload, so you should experiment with different values.\n",
    "    * Set the data loader's `prefetch_factor` argument to control the number of batches that each worker pre-fetches.\n",
    "    * If spawning and synchronizing workers causes too much overhead (especially on Windows), you can try setting `persistent_workers=True` to reuse the same workers across epochs.\n",
    "\n",
    "10. The main classification losses provided by PyTorch are:\n",
    "    * `nn.CrossEntropyLoss`: this is generally the loss you want to use for multiclass classification, as it's efficient and numerically stable. This loss works directly on logits, not probabilities, so your model must not include the softmax activation function on the output layer. As a result, whenever you need to estimate probabilities, you must call the `F.softmax()` function on the logits output by the model.\n",
    "    * `nn.BCEWithLogitsLoss` (BCE stands for _binary cross-entropy_): this is usually the loss you want to use for binary classification, for the same reason as `nn.CrossEntropyLoss`. Just like the previous loss, it works directly with logits, so your model must not include the sigmoid activation function on the output layer. Whenever you need to estimate probabilities, you must call the `F.sigmoid()` function on the logits output by the model. Note that some people prefer to use `nn.CrossEntropyLoss` even for binary classification, as it makes the code more consistent regardless of the number of classes, at a tiny computational and memory cost.\n",
    "    * `nn.NLLLoss` (NLL stands for _negative log-likelihood_): this is an alternative to `nn.CrossEntropyLoss` for multiclass classification. To use it, your model must output log probabilities rather than logits. This can be done using `nn.LogSoftmax()` or `F.log_softmax()`. This approach is a bit slower than using `CrossEntropyLoss`, but it can be useful if you want your model to output log probabilities rather than logits, or when you wish to tweak the probability distribution before computing the final loss (indeed, it is sometimes easier to modify log probabilities rather than logits). Whenever you need to estimate probabilities, you must call `torch.exp()` on the model's outputs.\n",
    "    * `nn.BCELoss`: this loss is an alternative to `nn.BCEWithLogitsLoss` for binary classification. It assumes that your model outputs probabilities rather than logits, so your model's output layer must use the sigmoid activation function. This can be convenient if you want your model to output probabilities directly, but it's a bit slower and less numerically stable than using `BCEWithLogitsLoss`.\n",
    "\n",
    "11. Calling `model.train()` before training and `model.eval()` before evaluation is important because some layers (such as `nn.Dropout`, `nn.BatchNorm1d` or `nn.BatchNorm2d`) don't behave in the same way during training and evaluation, therefore we must tell the model in which mode it should run.\n",
    "\n",
    "12. Both `torch.jit.trace()` and `torch.jit.script()` attempt to capture your model's computation graph and turn it into TorchScript code that can be optimized, saved, and deployed to various platforms. However, these functions work very differently:\n",
    "    * The `torch.jit.trace()` function runs your model with a tracing tensor that captures which operations are executed. It's quite simple and works well for simple models, but it cannot capture conditionals (e.g., `if`, `elif`, `else`, `match`): it only captures the branch of the conditional that is actually executed during tracing. Similarly, if your model contains a loop (e.g., `for` or `while`) then tracing will not capture the loop itself, it will only capture the repeated operations.\n",
    "    * The`torch.jit.script()` function actually parses your Python code to generate TorchScript code. This allows it to detect conditionals (as long as the conditions are tensors), and also capture loops. However, it only works with a subset of Python: you cannot use global variables, Python generators (`yield`), complex list comprehensions, variable length function arguments (`*args` or `**kwargs`), or `match` statements. Moreover, types must be fixed (a function cannot return an integer in some cases and a float in others), and you can only call other functions if they also respect these rules, so no standard library, no third-party libraries, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: _Use autograd to find the gradient vector of f(_x_, _y_) = sin(_x_<sup>2</sup> _y_) at the point (_x_, _y_) = (1.2, 3.4)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.489864706993103, 0.26291730999946594)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    return torch.sin(x ** 2 * y)\n",
    "\n",
    "x = torch.tensor(1.2, requires_grad=True)\n",
    "y = torch.tensor(3.4, requires_grad=True)\n",
    "result = f(x, y)\n",
    "result.backward()\n",
    "\n",
    "x.grad.item(), y.grad.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use a vectorized implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4899, 0.2629])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(v):\n",
    "    return torch.sin(v[0] ** 2 * v[1])\n",
    "\n",
    "v = torch.tensor([1.2, 3.4], requires_grad=True)\n",
    "result = f(v)\n",
    "result.backward()\n",
    "\n",
    "v.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same partial derivatives, but this time they are wrapped in a gradient tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check this result by computing an approximation of the partial derivatives, using the fact that the partial derivative of a function $f(x, y)$ with regard to $x$, at a point $(x_0, y_0)$ is the limit of $\\dfrac{f(x_0 + \\epsilon, y_0) - f(x_0, y0)}{\\epsilon}$ as $\\epsilon$ approaches zero. Similarly, the partial derivative of that function with regard to $y$, at the same point $(x_0, y_0)$, is the limit of $\\dfrac{f(x_0, y_0 + \\epsilon) - f(x_0, y0)}{\\epsilon}$ as $\\epsilon$ approaches zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.4901161193847656, 0.26226043701171875)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 0.00005\n",
    "df_dx = (f(x + eps, y) - f(x, y)) / eps\n",
    "df_dy = (f(x, y + eps) - f(x, y)) / eps\n",
    "\n",
    "df_dx.item(), df_dy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's close enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also derive the equations for the partial derivatives mathematically (see the [calculus tutorial](math_differential_calculus.ipynb) to learn how to do this):\n",
    "* $\\dfrac{\\partial f}{\\partial x} = 2xy \\cos(x^2 y)$\n",
    "* $\\dfrac{\\partial f}{\\partial x} = x^2 \\cos(x^2 y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.4898648262023926, 0.26291730999946594)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dx = 2 * x * y * torch.cos(x**2 * y)\n",
    "df_dy = x ** 2 * torch.cos(x**2 * y)\n",
    "\n",
    "df_dx.item(), df_dy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: _Create a custom `Dense` module that replicates the functionality of an `nn.Linear` module followed by an `nn.ReLU` module. Try implementing it first using the `nn.Linear` and `nn.ReLU` modules, and then reimplement it using `nn.Parameter` and the `relu()` function._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.relu(self.linear(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dense = Dense(3, 5)\n",
    "X = torch.randn(2, 3)\n",
    "y_pred = dense(X)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks fine. We can double-check that it gives the right result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_check = dense.relu(X @ dense.linear.weight.T + dense.linear.bias)\n",
    "torch.allclose(y_pred, y_pred_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. Now let's reimplement the `Dense` class using `nn.Parameter` and `relu()` instead of `nn.Linear` and `nn.ReLU`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense2(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "\n",
    "    def forward(self, X):\n",
    "        z = X @ self.weight.T + self.bias\n",
    "        return F.relu(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dense2 = Dense2(3, 5)\n",
    "X = torch.randn(2, 3)\n",
    "y_pred2 = dense2(X)\n",
    "y_pred2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks fine. Again, we can double-check that it gives the right result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred2_check = F.relu(X @ dense2.weight.T + dense2.bias)\n",
    "torch.allclose(y_pred2, y_pred2_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter 11, we will see that it's preferable to initialize the weights using Kaiming initialization, when the activation function is ReLU, so let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense3(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        nn.init.kaiming_uniform_(self.weight, nonlinearity=\"relu\")\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "\n",
    "    def forward(self, X):\n",
    "        z = X @ self.weight.T + self.bias\n",
    "        return F.relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dense3 = Dense3(3, 5)\n",
    "X = torch.randn(2, 3)\n",
    "y_pred3 = dense3(X)\n",
    "y_pred3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still works fine. Again, we can double-check that this module gives the right result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred3_check = F.relu(X @ dense3.weight.T + dense3.bias)\n",
    "torch.allclose(y_pred3, y_pred3_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: _Build and train a classification MLP on the CoverType dataset._\n",
    "\n",
    "Step 1: _Load the dataset using `sklearn.datasets.fetch_covtype()` and create a custom PyTorch `Dataset` for this data._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: do not forget to scale the inputs, as gradient descent might not work well otherwise (as we saw in Chapter 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "covtype = fetch_covtype()\n",
    "\n",
    "X_covtype = torch.tensor(covtype.data, dtype=torch.float32)\n",
    "means = X_covtype.mean(dim=0, keepdim=True)\n",
    "stds = X_covtype.std(dim=0, keepdim=True)\n",
    "X_standardized_covtype = (X_covtype - means) / stds\n",
    "\n",
    "y_covtype = torch.tensor(covtype.target - 1, dtype=torch.long)\n",
    "\n",
    "covtype_dataset = TensorDataset(X_standardized_covtype, y_covtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the targets range from 1 to 7, but the `nn.CrossEntropyLoss` expects it to start at 0, which is why we subtract 1 from the targets. We also convert the inputs from 64-bit floats to 32-bit floats, and the targets to 64-bit integers. These are the default types in PyTorch for floats and integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([54]), torch.Size([]))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample0, target0 = covtype_dataset[0]\n",
    "sample0.shape, target0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: _Create data loaders for training, validation, and testing._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is not shuffled or split by default, so we shuffle and split it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_size = len(covtype_dataset) * 80 // 100\n",
    "valid_size = len(covtype_dataset) * 10 // 100\n",
    "test_size = len(covtype_dataset) - train_size - valid_size\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(\n",
    "    covtype_dataset,\n",
    "    [train_size, valid_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: _Build a custom MLP module to tackle this classification task. You can optionally use the custom `Dense` module from the previous exercise._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to do this. Let's look at a few alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're building an MLP, which is just a stack of layers, we can just use a `nn.Sequential` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = len(covtype.feature_names)  # == 54\n",
    "n_classes = len(set(covtype.target))  # == 7\n",
    "\n",
    "torch.manual_seed(42)\n",
    "covtype_model = nn.Sequential(\n",
    "    nn.Linear(n_inputs, 200), nn.ReLU(),\n",
    "    nn.Linear(200, 100), nn.ReLU(),\n",
    "    nn.Linear(100, 50), nn.ReLU(),\n",
    "    nn.Linear(50, n_classes)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output layer must not use any activation function since we will use the `nn.CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use the `Dense3` class we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "covtype_model = nn.Sequential(\n",
    "    Dense3(n_inputs, 200),\n",
    "    Dense3(200, 100),\n",
    "    Dense3(100, 50),\n",
    "    nn.Linear(50, n_classes)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can create a custom module. Let's make it flexible so it accepts a list containing the number of neurons in each hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverTypeModel(nn.Module):\n",
    "    def __init__(self, n_neurons, n_inputs=n_inputs, n_classes=n_classes):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            Dense3(n_in, n_out)\n",
    "            for n_in, n_out in zip([n_inputs] + n_neurons, n_neurons)\n",
    "        ] + [nn.Linear(n_neurons[-1], n_classes)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mlp(X)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "covtype_model = CoverTypeModel([200, 100, 50]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we could use a `nn.ModuleList` instead of a `nn.Sequential` module, and modify the `forward()` method to explicitly run each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverTypeModel(nn.Module):\n",
    "    def __init__(self, n_neurons, n_inputs=n_inputs, n_classes=n_classes):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            Dense3(n_in, n_out)\n",
    "            for n_in, n_out in zip([n_inputs] + n_neurons, n_neurons)\n",
    "        ] + [nn.Linear(n_neurons[-1], n_classes)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: _Train this model on the GPU, and try to reach 93% accuracy on the test set. For this, you will likely have to perform hyperparameter search to find the right number of layers and neurons per layer, a good learning rate and batch size, and so on. You can optionally use Optuna for this._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model on the GPU. As we saw in Chapter 4, it's often helpful to reduce the learning rate at the end of training, so let's do that. We will train for 6 times 15 epochs, reducing the learning rate each time. The results might vary depending on the platform, but you should get over 94% accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, train loss: 0.5831, train metric: 0.7497, valid metric: 0.7917\n",
      "Epoch 2/15, train loss: 0.4597, train metric: 0.8037, valid metric: 0.8138\n",
      "Epoch 3/15, train loss: 0.4057, train metric: 0.8284, valid metric: 0.8317\n",
      "Epoch 4/15, train loss: 0.3706, train metric: 0.8447, valid metric: 0.8384\n",
      "Epoch 5/15, train loss: 0.3447, train metric: 0.8568, valid metric: 0.8638\n",
      "Epoch 6/15, train loss: 0.3237, train metric: 0.8665, valid metric: 0.8642\n",
      "Epoch 7/15, train loss: 0.3078, train metric: 0.8735, valid metric: 0.8720\n",
      "Epoch 8/15, train loss: 0.2945, train metric: 0.8793, valid metric: 0.8845\n",
      "Epoch 9/15, train loss: 0.2830, train metric: 0.8839, valid metric: 0.8786\n",
      "Epoch 10/15, train loss: 0.2737, train metric: 0.8881, valid metric: 0.8675\n",
      "Epoch 11/15, train loss: 0.2654, train metric: 0.8916, valid metric: 0.8917\n",
      "Epoch 12/15, train loss: 0.2561, train metric: 0.8959, valid metric: 0.8846\n",
      "Epoch 13/15, train loss: 0.2513, train metric: 0.8982, valid metric: 0.8910\n",
      "Epoch 14/15, train loss: 0.2443, train metric: 0.9008, valid metric: 0.8954\n",
      "Epoch 15/15, train loss: 0.2380, train metric: 0.9037, valid metric: 0.8874\n",
      "Epoch 1/15, train loss: 0.1995, train metric: 0.9201, valid metric: 0.9124\n",
      "Epoch 2/15, train loss: 0.1946, train metric: 0.9223, valid metric: 0.9174\n",
      "Epoch 3/15, train loss: 0.1918, train metric: 0.9234, valid metric: 0.9155\n",
      "Epoch 4/15, train loss: 0.1892, train metric: 0.9241, valid metric: 0.9168\n",
      "Epoch 5/15, train loss: 0.1874, train metric: 0.9246, valid metric: 0.9207\n",
      "Epoch 6/15, train loss: 0.1853, train metric: 0.9260, valid metric: 0.9209\n",
      "Epoch 7/15, train loss: 0.1831, train metric: 0.9266, valid metric: 0.9191\n",
      "Epoch 8/15, train loss: 0.1817, train metric: 0.9270, valid metric: 0.9224\n",
      "Epoch 9/15, train loss: 0.1802, train metric: 0.9279, valid metric: 0.9237\n",
      "Epoch 10/15, train loss: 0.1784, train metric: 0.9283, valid metric: 0.9197\n",
      "Epoch 11/15, train loss: 0.1766, train metric: 0.9292, valid metric: 0.9195\n",
      "Epoch 12/15, train loss: 0.1753, train metric: 0.9297, valid metric: 0.9203\n",
      "Epoch 13/15, train loss: 0.1745, train metric: 0.9299, valid metric: 0.9193\n",
      "Epoch 14/15, train loss: 0.1724, train metric: 0.9309, valid metric: 0.9224\n",
      "Epoch 15/15, train loss: 0.1716, train metric: 0.9312, valid metric: 0.9253\n",
      "Epoch 1/15, train loss: 0.1480, train metric: 0.9415, valid metric: 0.9354\n",
      "Epoch 2/15, train loss: 0.1455, train metric: 0.9423, valid metric: 0.9318\n",
      "Epoch 3/15, train loss: 0.1443, train metric: 0.9432, valid metric: 0.9342\n",
      "Epoch 4/15, train loss: 0.1431, train metric: 0.9436, valid metric: 0.9350\n",
      "Epoch 5/15, train loss: 0.1425, train metric: 0.9436, valid metric: 0.9340\n",
      "<<20 more lines>>\n",
      "Epoch 11/15, train loss: 0.1171, train metric: 0.9544, valid metric: 0.9446\n",
      "Epoch 12/15, train loss: 0.1168, train metric: 0.9545, valid metric: 0.9446\n",
      "Epoch 13/15, train loss: 0.1162, train metric: 0.9544, valid metric: 0.9436\n",
      "Epoch 14/15, train loss: 0.1159, train metric: 0.9546, valid metric: 0.9446\n",
      "Epoch 15/15, train loss: 0.1156, train metric: 0.9546, valid metric: 0.9449\n",
      "Epoch 1/15, train loss: 0.1087, train metric: 0.9581, valid metric: 0.9470\n",
      "Epoch 2/15, train loss: 0.1081, train metric: 0.9586, valid metric: 0.9472\n",
      "Epoch 3/15, train loss: 0.1076, train metric: 0.9591, valid metric: 0.9466\n",
      "Epoch 4/15, train loss: 0.1073, train metric: 0.9588, valid metric: 0.9470\n",
      "Epoch 5/15, train loss: 0.1072, train metric: 0.9591, valid metric: 0.9477\n",
      "Epoch 6/15, train loss: 0.1068, train metric: 0.9591, valid metric: 0.9476\n",
      "Epoch 7/15, train loss: 0.1067, train metric: 0.9593, valid metric: 0.9466\n",
      "Epoch 8/15, train loss: 0.1063, train metric: 0.9594, valid metric: 0.9473\n",
      "Epoch 9/15, train loss: 0.1062, train metric: 0.9595, valid metric: 0.9468\n",
      "Epoch 10/15, train loss: 0.1058, train metric: 0.9596, valid metric: 0.9468\n",
      "Epoch 11/15, train loss: 0.1057, train metric: 0.9598, valid metric: 0.9481\n",
      "Epoch 12/15, train loss: 0.1056, train metric: 0.9598, valid metric: 0.9475\n",
      "Epoch 13/15, train loss: 0.1053, train metric: 0.9598, valid metric: 0.9466\n",
      "Epoch 14/15, train loss: 0.1050, train metric: 0.9598, valid metric: 0.9478\n",
      "Epoch 15/15, train loss: 0.1049, train metric: 0.9600, valid metric: 0.9473\n",
      "Epoch 1/15, train loss: 0.1014, train metric: 0.9616, valid metric: 0.9493\n",
      "Epoch 2/15, train loss: 0.1009, train metric: 0.9617, valid metric: 0.9488\n",
      "Epoch 3/15, train loss: 0.1008, train metric: 0.9620, valid metric: 0.9496\n",
      "Epoch 4/15, train loss: 0.1007, train metric: 0.9620, valid metric: 0.9485\n",
      "Epoch 5/15, train loss: 0.1006, train metric: 0.9620, valid metric: 0.9496\n",
      "Epoch 6/15, train loss: 0.1004, train metric: 0.9623, valid metric: 0.9495\n",
      "Epoch 7/15, train loss: 0.1003, train metric: 0.9622, valid metric: 0.9494\n",
      "Epoch 8/15, train loss: 0.1001, train metric: 0.9622, valid metric: 0.9492\n",
      "Epoch 9/15, train loss: 0.1001, train metric: 0.9622, valid metric: 0.9488\n",
      "Epoch 10/15, train loss: 0.0999, train metric: 0.9623, valid metric: 0.9495\n",
      "Epoch 11/15, train loss: 0.0998, train metric: 0.9625, valid metric: 0.9491\n",
      "Epoch 12/15, train loss: 0.0997, train metric: 0.9624, valid metric: 0.9494\n",
      "Epoch 13/15, train loss: 0.0996, train metric: 0.9623, valid metric: 0.9496\n",
      "Epoch 14/15, train loss: 0.0995, train metric: 0.9626, valid metric: 0.9501\n",
      "Epoch 15/15, train loss: 0.0993, train metric: 0.9626, valid metric: 0.9490\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "covtype_model = CoverTypeModel([200, 100, 50]).to(device)\n",
    "\n",
    "for learning_rate in [0.16, 0.08, 0.04, 0.02, 0.01, 0.005]:\n",
    "    n_epochs = 15\n",
    "    optimizer = torch.optim.SGD(covtype_model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    metric = torchmetrics.Accuracy(task=\"multiclass\",\n",
    "                                   num_classes=n_classes).to(device)\n",
    "    history = train2(covtype_model, optimizer, criterion, metric, train_loader,\n",
    "                     valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9488, device='cuda:0')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_tm(covtype_model, test_loader, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I manually tuned the hyperparameters above through trial and error. It took about 30 minutes of search to achieve over 94% accuracy, so I didn’t need to run a full hyperparameter search. However, you can definitely use Optuna for this task if you want to get even better results. The code below tunes the learning rate, the number of hidden layers, and the number of neurons per hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 1.0, log=True)\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    n_hidden = trial.suggest_int(\"n_hidden\", 30, 150)\n",
    "    covtype_model = CoverTypeModel([n_hidden] * n_layers).to(device)\n",
    "    optimizer = torch.optim.SGD(covtype_model.parameters(), lr=learning_rate)\n",
    "    xentropy = nn.CrossEntropyLoss()\n",
    "    accuracy = torchmetrics.Accuracy(task=\"multiclass\",\n",
    "                                     num_classes=n_classes).to(device)\n",
    "    best_validation_accuracy = 0.0\n",
    "    for epoch in range(n_epochs):\n",
    "        history = train2(covtype_model, optimizer, xentropy, accuracy,\n",
    "                         train_loader, valid_loader, n_epochs=1)\n",
    "        validation_accuracy = max(history[\"valid_metrics\"])\n",
    "        if validation_accuracy > best_validation_accuracy:\n",
    "            best_validation_accuracy = validation_accuracy\n",
    "        trial.report(validation_accuracy, step=epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    return best_validation_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: I’ve set `n_trials=2` so you can run a quick test, but if you want to actually tune the hyperparameters, you should increase this to at least 50—and ideally to several hundred, but it will take several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-10 21:19:34,312] A new study created in memory with name: no-name-a48459d0-5e94-406c-88f9-e1bc884a62f9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, train loss: 0.6379, train metric: 0.7310, valid metric: 0.7674\n",
      "Epoch 1/1, train loss: 0.5245, train metric: 0.7765, valid metric: 0.7928\n",
      "Epoch 1/1, train loss: 0.4782, train metric: 0.7976, valid metric: 0.7992\n",
      "Epoch 1/1, train loss: 0.4444, train metric: 0.8131, valid metric: 0.8180\n",
      "Epoch 1/1, train loss: 0.4185, train metric: 0.8249, valid metric: 0.8197\n",
      "Epoch 1/1, train loss: 0.3978, train metric: 0.8341, valid metric: 0.8434\n",
      "Epoch 1/1, train loss: 0.3801, train metric: 0.8421, valid metric: 0.8480\n",
      "Epoch 1/1, train loss: 0.3635, train metric: 0.8498, valid metric: 0.8281\n",
      "Epoch 1/1, train loss: 0.3501, train metric: 0.8558, valid metric: 0.8549\n",
      "Epoch 1/1, train loss: 0.3385, train metric: 0.8613, valid metric: 0.8684\n",
      "Epoch 1/1, train loss: 0.3270, train metric: 0.8660, valid metric: 0.8718\n",
      "Epoch 1/1, train loss: 0.3170, train metric: 0.8704, valid metric: 0.8714\n",
      "Epoch 1/1, train loss: 0.3070, train metric: 0.8751, valid metric: 0.8676\n",
      "Epoch 1/1, train loss: 0.3015, train metric: 0.8771, valid metric: 0.8472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-10 21:20:29,002] Trial 0 finished with value: 0.8718438744544983 and parameters: {'learning_rate': 0.05611516415334506, 'n_layers': 3, 'n_hidden': 118}. Best is trial 0 with value: 0.8718438744544983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, train loss: 0.2921, train metric: 0.8814, valid metric: 0.8700\n",
      "Epoch 1/1, train loss: 0.6367, train metric: 0.7331, valid metric: 0.7501\n",
      "Epoch 1/1, train loss: 0.5668, train metric: 0.7583, valid metric: 0.7631\n",
      "Epoch 1/1, train loss: 0.5447, train metric: 0.7674, valid metric: 0.7765\n",
      "Epoch 1/1, train loss: 0.5293, train metric: 0.7748, valid metric: 0.7797\n",
      "Epoch 1/1, train loss: 0.5186, train metric: 0.7789, valid metric: 0.7738\n",
      "Epoch 1/1, train loss: 0.5109, train metric: 0.7830, valid metric: 0.7894\n",
      "Epoch 1/1, train loss: 0.5046, train metric: 0.7853, valid metric: 0.7897\n",
      "Epoch 1/1, train loss: 0.4998, train metric: 0.7879, valid metric: 0.7919\n",
      "Epoch 1/1, train loss: 0.4961, train metric: 0.7886, valid metric: 0.7948\n",
      "Epoch 1/1, train loss: 0.4925, train metric: 0.7907, valid metric: 0.7884\n",
      "Epoch 1/1, train loss: 0.4895, train metric: 0.7925, valid metric: 0.8016\n",
      "Epoch 1/1, train loss: 0.4865, train metric: 0.7935, valid metric: 0.7924\n",
      "Epoch 1/1, train loss: 0.4862, train metric: 0.7942, valid metric: 0.7899\n",
      "Epoch 1/1, train loss: 0.4824, train metric: 0.7949, valid metric: 0.7844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-10 21:21:15,807] Trial 1 finished with value: 0.8016385436058044 and parameters: {'learning_rate': 0.15751320499779728, 'n_layers': 1, 'n_hidden': 48}. Best is trial 0 with value: 0.8718438744544983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, train loss: 0.4792, train metric: 0.7962, valid metric: 0.7894\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "pruner = optuna.pruners.MedianPruner()\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler,\n",
    "                            pruner=pruner)\n",
    "study.optimize(objective, n_trials=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8718438744544983"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05611516415334506, 'n_layers': 3, 'n_hidden': 118}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope you enjoyed this chapter!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "handson-mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
